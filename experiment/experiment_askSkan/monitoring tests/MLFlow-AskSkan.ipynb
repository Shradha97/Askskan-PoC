{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.7.4-cp39-cp39-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Users/saishradhamohanty/Desktop/Repo/Prototypes/.mlflow/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ! pip install tiktoken\n",
    "# ! pip install python-dotenv\n",
    "# ! pip install pandas\n",
    "# ! pip install pandasql\n",
    "# ! pip install openai\n",
    "# ! pip install langchain\n",
    "# ! pip install langchain\\[all\\]\n",
    "# ! pip install pyspark\n",
    "# ! pip install findspark\n",
    "# ! pip install faker\n",
    "\n",
    "# !pip3 install 'mlflow[gateway]'\n",
    "!pip3 install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLFlow Routing Gateway API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/saishradhamohanty/Desktop/Repo/Prototypes/.mlflow/lib/python3.9/site-packages/pydantic/_internal/_fields.py:149: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/saishradhamohanty/Desktop/Repo/Prototypes/.mlflow/lib/python3.9/site-packages/pydantic/_internal/_config.py:318: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n",
      "[2023-11-20 20:53:26 -0500] [26532] [INFO] Starting gunicorn 21.2.0\n",
      "[2023-11-20 20:53:26 -0500] [26532] [INFO] Listening at: http://127.0.0.1:5000 (26532)\n",
      "[2023-11-20 20:53:26 -0500] [26532] [INFO] Using worker: uvicorn.workers.UvicornWorker\n",
      "[2023-11-20 20:53:26 -0500] [26533] [INFO] Booting worker with pid: 26533\n",
      "[2023-11-20 20:53:26 -0500] [26534] [INFO] Booting worker with pid: 26534\n",
      "/Users/saishradhamohanty/Desktop/Repo/Prototypes/.mlflow/lib/python3.9/site-packages/pydantic/_internal/_fields.py:149: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/saishradhamohanty/Desktop/Repo/Prototypes/.mlflow/lib/python3.9/site-packages/pydantic/_internal/_config.py:318: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n",
      "/Users/saishradhamohanty/Desktop/Repo/Prototypes/.mlflow/lib/python3.9/site-packages/pydantic/_internal/_fields.py:149: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/saishradhamohanty/Desktop/Repo/Prototypes/.mlflow/lib/python3.9/site-packages/pydantic/_internal/_config.py:318: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n",
      "[2023-11-20 20:53:28 -0500] [26534] [INFO] Started server process [26534]\n",
      "[2023-11-20 20:53:28 -0500] [26533] [INFO] Started server process [26533]\n",
      "[2023-11-20 20:53:28 -0500] [26534] [INFO] Waiting for application startup.\n",
      "[2023-11-20 20:53:28 -0500] [26533] [INFO] Waiting for application startup.\n",
      "[2023-11-20 20:53:28 -0500] [26534] [INFO] Application startup complete.\n",
      "[2023-11-20 20:53:28 -0500] [26533] [INFO] Application startup complete.\n",
      "^C\n",
      "[2023-11-20 20:53:39 -0500] [26532] [INFO] Handling signal: int\n",
      "[2023-11-20 20:53:39 -0500] [26533] [INFO] Shutting down\n",
      "[2023-11-20 20:53:39 -0500] [26533] [INFO] Error while closing socket [Errno 9] Bad file descriptor\n",
      "[2023-11-20 20:53:39 -0500] [26534] [INFO] Shutting down\n",
      "[2023-11-20 20:53:39 -0500] [26534] [INFO] Error while closing socket [Errno 9] Bad file descriptor\n",
      "[2023-11-20 20:53:39 -0500] [26533] [INFO] Waiting for application shutdown.\n",
      "[2023-11-20 20:53:39 -0500] [26533] [INFO] Application shutdown complete.\n",
      "[2023-11-20 20:53:39 -0500] [26533] [INFO] Finished server process [26533]\n",
      "[2023-11-20 20:53:39 -0500] [26533] [INFO] Worker exiting (pid: 26533)\n",
      "[2023-11-20 20:53:39 -0500] [26534] [INFO] Waiting for application shutdown.\n",
      "[2023-11-20 20:53:39 -0500] [26534] [INFO] Application shutdown complete.\n",
      "[2023-11-20 20:53:39 -0500] [26534] [INFO] Finished server process [26534]\n",
      "[2023-11-20 20:53:39 -0500] [26534] [INFO] Worker exiting (pid: 26534)\n"
     ]
    }
   ],
   "source": [
    "# !mlflow gateway start --config-path /Users/saishradhamohanty/Desktop/Repo/Prototypes/askskan/monitoring/mlflow_config.yaml!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatMLflowAIGateway\n",
    "from langchain.embeddings import MlflowAIGatewayEmbeddings, AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.document_loaders.text import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallback_message = \"Sorry, I'm not intelligent enough to answer your question now. Please try again or with a different question!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Azure keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "api_type = 'azure'\n",
    "model_deployment_name=os.getenv(\"AZURE_OPENAI_DNAME\")\n",
    "\n",
    "embeddings_deployment_name= os.getenv(\"AZURE_OPENAI_EMBED_NAME\") # os.getenv(\"AZURE_OPENAI_EMBED_NAME\")\n",
    "embedding_api_version='2022-12-01'\n",
    "model_api_version='2023-05-15'\n",
    "\n",
    "max_tokens = 1000\n",
    "chat_temperature = 0\n",
    " #This will correspond to the custom name you chose for your deployment when you deployed a model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read schema in JSON format\n",
    "Why only JSON?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = '/Users/saishradhamohanty/Desktop/Repo/Prototypes/askskan/data/original/input_tables/events/schema.csv'\n",
    "\n",
    "data_file1 = '/Users/saishradhamohanty/Desktop/Repo/Prototypes/askskan/data/sample/data0.csv'\n",
    "definition_text_file = '/Users/saishradhamohanty/Desktop/Repo/Prototypes/askskan/data/original/input_tables/events/definitions.txt'\n",
    "fake_data_file = '../askskan/data/original/sample_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLFlow variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_uri = \"http://127.0.0.1:5000\"\n",
    "temperature = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_csv = CSVLoader(file_path=file1)\n",
    "docs_csv = loader_csv.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\",\n",
    "#                               chunk_size=1)\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "                deployment=embeddings_deployment_name,\n",
    "                openai_api_key=api_key,\n",
    "                azure_endpoint=api_endpoint,\n",
    "                openai_api_type=api_type,\n",
    "                openai_api_version=embedding_api_version,\n",
    "                chunk_size=16,\n",
    "            )\n",
    "\n",
    "# To use the MLflow Gateway, have to separately query the MLflow server with each document element \\\n",
    "# for obtaining the embeddings\n",
    "# embeddings = MlflowAIGatewayEmbeddings(\n",
    "#     gateway_uri=mlflow_uri,\n",
    "#     route=\"embeddings\",\n",
    "#     chunk_size=16,\n",
    "# )\n",
    "\n",
    "#vectorstore = DocArrayInMemorySearch.from_documents(documents, embeddings)\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing verctor_db to a temporary location on local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    persist_dir = os.path.join(temp_dir, \"faiss_vectorstore\")\n",
    "    vectorstore.save_local(persist_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to load the saved vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_retriever(persist_directory, embeddings):\n",
    "        # embeddings = OpenAIEmbeddings()\n",
    "        vectorstore = FAISS.load_local(persist_directory, embeddings)\n",
    "        return vectorstore.as_retriever()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create template to provide as an input to the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_without_followups = \"\"\"\n",
    "You are a super smart code generator.\n",
    "Perform the following actions:\n",
    "\n",
    "1. Understand the question by human, schema, schema definitions and context each delimited by <>.\n",
    "2. Do not assume any schema attributes unless stated explicitly.\n",
    "3. Use columns in the schema to generate a final Spark SQL query from the table in data table name delimited by <>\\\n",
    "    answering the original question. The generated query should for data from the start date to end date\\\n",
    "        each delimited by <>. Refer to the schema definitions only for more clarity about terms in the schema.\n",
    "4. Output computationally most efficient Spark SQL query.\n",
    "5. Refer to the orginal question again and then select the correct column from the generated Spark SQL query.\n",
    "5. I have a pandas csv table in data table delimited by <> that contains the data to be queried.\n",
    "    - Output a valid python code to execute the generated Spark SQL query on the data file.\n",
    "6. Omit any explanations.\n",
    "\n",
    "Schema: <{context}>\n",
    "Schema definitions: <{schema_definitions}>\n",
    "Data table: <{data_table}>\n",
    "Data table name: <{data_table_name}>\n",
    "Start date: <{start_date}>\n",
    "End date: <{end_date}>\n",
    "\n",
    "\n",
    "Once the Spark SQL query generated use only the following JSON schema format stricly, refer to the example output:\n",
    "Output: {{\n",
    "    \"Query\": The generated Spark SQL query from the start date to end date,\n",
    "    \"Column\": The correct extracted column from the Spark SQL query,\n",
    "    \"Code\": Python code generated to run the Spark SQL query. The sql query within this python code should be \\\n",
    "       only be within single quotes e.g. query='SELECT * FROM table' ,\n",
    "    \"Skan Bot\": The final answer printed by the python code in a friendly tone. Delimit the answer obtained by \\\n",
    "        python code in ##.\n",
    "}}\n",
    "\n",
    "Following is an example of the output:\n",
    "Output: {{\n",
    "    \"Query\": \"SELECT app_name, COUNT(*) AS count FROM clipboard WHERE event_date >= '2023-04-01' AND event_date <= '2023-04-30'\" \\\n",
    "        GROUP BY app_name ORDER BY count DESC LIMIT 1',\n",
    "    \"Column\": \"app_name\", \n",
    "    \"Code\": \"import pandas as pd\\nfrom pyspark.sql import SparkSession\\n\\nspark = SparkSession.builder.appName('schema').\\\n",
    "        getOrCreate()\\ndata = spark.read.csv('../askskan/data/original/sample_data.csv', header=True, inferSchema=True)\\ndata.\\\n",
    "        createOrReplaceTempView('clipboard')\\n\\nquery = 'SELECT app_name, COUNT(*) AS count FROM clipboard GROUP BY app_name \\\n",
    "        ORDER BY count DESC LIMIT 1'\\nresult = spark.sql(query)\\n\\n spark.stop()\\n\",\n",
    "    \"Skan Bot\": \"The most used application is #result#.\"\n",
    "}}\n",
    "\n",
    "nsure the Output can be parsed by Python json.loads\n",
    "    \n",
    "\n",
    "Current conversation:\n",
    "{chat_history}   \n",
    "Human: <{question}>\n",
    "Skan Bot: \n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions to be asked by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = \"Total time spent on process application?\"\n",
    "question_2 = \"Total time spent of non process application?\"\n",
    "question_3 = \"Is there an outstanding performer in CES persona?\"\n",
    "question_4 = \"Which is the most used application?\"\n",
    "question_5 = \"Which is the most active time window during the day?\"\n",
    "question_6 = \"What is the average case effort per persona?\"\n",
    "question_7 = \"What is the average utilization per persona?\"\n",
    "question_8 = \"Which week had highest productivity in CES?\"\n",
    "question_9 = \"What is the distribution of time spent in processing application per day?\"\n",
    "question_10 = \"What is the average number of participants per case?\"\n",
    "question_11 = \"What is the frequency of cases per participant per day?\"\n",
    "question_12 = \"Who will be most productive next week?\"\n",
    "question_13 = \"Any patterns observed that give an insight on inefficiencies?\"\n",
    "question_14 = \"Would taking breaks improve the efficiency in performance?\"\n",
    "\n",
    "question_15 = \"What is the standard deviation of time spent on process application?\"\n",
    "question_16 = \"What is the standard deviation of time spent on non process application?\"\n",
    "\n",
    "# question_1 = f\"\"\"Who has the longest average processing time?\"\"\"\n",
    "# question_2 = f\"\"\"Who has the shortest average processing time?\"\"\"\n",
    "# question_3 = f\"\"\"Who is the most productive participant based on the number of tasks completed?\"\"\"\n",
    "# question_4 = f\"\"\"Who uses the least number of keystrokes on average?\"\"\"\n",
    "# question_5 = f\"\"\"Name of the participant that uses the most number of long cut keys on average?\"\"\" # An invalid question, to check hallucination\n",
    "exception_question = f\"\"\"How was the day?\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with Conversational Retrieval QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the text file\n",
    "file_path = definition_text_file\n",
    "\n",
    "# Read the contents of the text file\n",
    "with open(file_path, 'r') as file:\n",
    "    schema_definitions = file.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Azure LLM - Now using the one present in the MLFlow API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LangChain with Azure OpenAI\n",
    "chat_llm = AzureChatOpenAI(\n",
    "    deployment_name=model_deployment_name,\n",
    "    openai_api_version=model_api_version,\n",
    "    azure_endpoint=api_endpoint,\n",
    "    openai_api_key=api_key,\n",
    "    # max_tokens=max_tokens,\n",
    "    temperature=0.3,\n",
    "    streaming=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chat_llm_api_version = ChatMLflowAIGateway(\n",
    "    gateway_uri=mlflow_uri,\n",
    "    route=\"chat_35-turbo\",\n",
    "    params={\n",
    "        \"temperature\": temperature,\n",
    "        \"streaming\": True,\n",
    "        \"verbose\": True,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "# code_parser = PydanticOutputParser(pydantic_object=FormatCodeOutput)\n",
    "# doubt_parser = PydanticOutputParser(pydantic_object=FormatDoubtOutput)\n",
    "\n",
    "# new_prompt = PromptTemplate(\n",
    "#     template=new_template_with_code, \n",
    "#     input_variables=[\"context\", \"question\", \"chat_history\"] , #\"fallback_message\"]\n",
    "#     partial_variables={\"schema_definitions\": schema_definitions,\n",
    "#                        \"data_table\": fake_data_file}\n",
    "# )\n",
    "\n",
    "new_prompt = PromptTemplate(\n",
    "    template=prompt_without_followups,\n",
    "    input_variables=[\n",
    "        \"context\",\n",
    "        \"question\",\n",
    "        \"chat_history\",\n",
    "    ],\n",
    "    partial_variables={\"schema_definitions\": schema_definitions,\n",
    "                        \"data_table\": fake_data_file,\n",
    "                        \"data_table_name\": \"hive_metastore.unum_askskan.events\",\n",
    "                        \"start_date\": \"2023-04-01\",\n",
    "                        \"end_date\": \"2023-04-30\"}\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": new_prompt}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the chat history separately as a parameter to MLFlow\n",
    "buffer_memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\")\n",
    "qa_api_version = ConversationalRetrievalChain.from_llm(\n",
    "    llm = chat_llm, # chat_llm_api_version, \n",
    "    retriever = vectorstore.as_retriever(), \n",
    "    memory=buffer_memory,     # Memory does not get included when logging the model to MLflow\n",
    "    combine_docs_chain_kwargs=chain_type_kwargs,\n",
    "    verbose=False,\n",
    "    get_chat_history=None # lambda h:h\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging the model in MLFLow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/24 14:03:49 WARNING mlflow: MLflow does not guarantee support for Chains outside of the subclasses of LLMChain, found ConversationalRetrievalChain\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Saving of memory is not yet supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/saishradhamohanty/Desktop/Repo/Prototypes/experiment/experiment_askSkan/monitoring tests/MLFlow-AskSkan.ipynb Cell 38\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saishradhamohanty/Desktop/Repo/Prototypes/experiment/experiment_askSkan/monitoring%20tests/MLFlow-AskSkan.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run() \u001b[39mas\u001b[39;00m run:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saishradhamohanty/Desktop/Repo/Prototypes/experiment/experiment_askSkan/monitoring%20tests/MLFlow-AskSkan.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         chat_history \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m h: h\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/saishradhamohanty/Desktop/Repo/Prototypes/experiment/experiment_askSkan/monitoring%20tests/MLFlow-AskSkan.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         logged_model \u001b[39m=\u001b[39m mlflow\u001b[39m.\u001b[39;49mlangchain\u001b[39m.\u001b[39;49mlog_model(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saishradhamohanty/Desktop/Repo/Prototypes/experiment/experiment_askSkan/monitoring%20tests/MLFlow-AskSkan.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m             qa_api_version,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saishradhamohanty/Desktop/Repo/Prototypes/experiment/experiment_askSkan/monitoring%20tests/MLFlow-AskSkan.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m             artifact_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mconversational_retrieval_qa_api_version\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saishradhamohanty/Desktop/Repo/Prototypes/experiment/experiment_askSkan/monitoring%20tests/MLFlow-AskSkan.ipynb#X52sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m             loader_fn\u001b[39m=\u001b[39;49mload_retriever,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saishradhamohanty/Desktop/Repo/Prototypes/experiment/experiment_askSkan/monitoring%20tests/MLFlow-AskSkan.ipynb#X52sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m             persist_dir\u001b[39m=\u001b[39;49mpersist_dir,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saishradhamohanty/Desktop/Repo/Prototypes/experiment/experiment_askSkan/monitoring%20tests/MLFlow-AskSkan.ipynb#X52sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.mlflow/lib/python3.9/site-packages/mlflow/langchain/__init__.py:539\u001b[0m, in \u001b[0;36mlog_model\u001b[0;34m(lc_model, artifact_path, conda_env, code_paths, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, metadata, loader_fn, persist_dir)\u001b[0m\n\u001b[1;32m    535\u001b[0m         output_schema \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    537\u001b[0m     signature \u001b[39m=\u001b[39m ModelSignature(input_schema, output_schema)\n\u001b[0;32m--> 539\u001b[0m \u001b[39mreturn\u001b[39;00m Model\u001b[39m.\u001b[39;49mlog(\n\u001b[1;32m    540\u001b[0m     artifact_path\u001b[39m=\u001b[39;49martifact_path,\n\u001b[1;32m    541\u001b[0m     flavor\u001b[39m=\u001b[39;49mmlflow\u001b[39m.\u001b[39;49mlangchain,\n\u001b[1;32m    542\u001b[0m     registered_model_name\u001b[39m=\u001b[39;49mregistered_model_name,\n\u001b[1;32m    543\u001b[0m     lc_model\u001b[39m=\u001b[39;49mlc_model,\n\u001b[1;32m    544\u001b[0m     conda_env\u001b[39m=\u001b[39;49mconda_env,\n\u001b[1;32m    545\u001b[0m     code_paths\u001b[39m=\u001b[39;49mcode_paths,\n\u001b[1;32m    546\u001b[0m     signature\u001b[39m=\u001b[39;49msignature,\n\u001b[1;32m    547\u001b[0m     input_example\u001b[39m=\u001b[39;49minput_example,\n\u001b[1;32m    548\u001b[0m     await_registration_for\u001b[39m=\u001b[39;49mawait_registration_for,\n\u001b[1;32m    549\u001b[0m     pip_requirements\u001b[39m=\u001b[39;49mpip_requirements,\n\u001b[1;32m    550\u001b[0m     extra_pip_requirements\u001b[39m=\u001b[39;49mextra_pip_requirements,\n\u001b[1;32m    551\u001b[0m     metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    552\u001b[0m     loader_fn\u001b[39m=\u001b[39;49mloader_fn,\n\u001b[1;32m    553\u001b[0m     persist_dir\u001b[39m=\u001b[39;49mpersist_dir,\n\u001b[1;32m    554\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.mlflow/lib/python3.9/site-packages/mlflow/models/model.py:619\u001b[0m, in \u001b[0;36mModel.log\u001b[0;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    614\u001b[0m     (tracking_uri \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdatabricks\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m get_uri_scheme(tracking_uri) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdatabricks\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    615\u001b[0m     \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39msignature\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    616\u001b[0m     \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39minput_example\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    617\u001b[0m ):\n\u001b[1;32m    618\u001b[0m     _logger\u001b[39m.\u001b[39mwarning(_LOG_MODEL_MISSING_SIGNATURE_WARNING)\n\u001b[0;32m--> 619\u001b[0m flavor\u001b[39m.\u001b[39;49msave_model(path\u001b[39m=\u001b[39;49mlocal_path, mlflow_model\u001b[39m=\u001b[39;49mmlflow_model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    620\u001b[0m mlflow\u001b[39m.\u001b[39mtracking\u001b[39m.\u001b[39mfluent\u001b[39m.\u001b[39mlog_artifacts(local_path, mlflow_model\u001b[39m.\u001b[39martifact_path)\n\u001b[1;32m    621\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.mlflow/lib/python3.9/site-packages/mlflow/langchain/__init__.py:278\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(lc_model, path, conda_env, code_paths, mlflow_model, signature, input_example, pip_requirements, extra_pip_requirements, metadata, loader_fn, persist_dir)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m metadata \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    276\u001b[0m     mlflow_model\u001b[39m.\u001b[39mmetadata \u001b[39m=\u001b[39m metadata\n\u001b[0;32m--> 278\u001b[0m model_data_kwargs \u001b[39m=\u001b[39m _save_model(lc_model, path, loader_fn, persist_dir)\n\u001b[1;32m    280\u001b[0m pyfunc\u001b[39m.\u001b[39madd_to_model(\n\u001b[1;32m    281\u001b[0m     mlflow_model,\n\u001b[1;32m    282\u001b[0m     loader_module\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmlflow.langchain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_data_kwargs,\n\u001b[1;32m    287\u001b[0m )\n\u001b[1;32m    288\u001b[0m flavor_conf \u001b[39m=\u001b[39m {\n\u001b[1;32m    289\u001b[0m     _MODEL_TYPE_KEY: lc_model\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m,\n\u001b[1;32m    290\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_data_kwargs,\n\u001b[1;32m    291\u001b[0m }\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.mlflow/lib/python3.9/site-packages/mlflow/langchain/__init__.py:625\u001b[0m, in \u001b[0;36m_save_model\u001b[0;34m(model, path, loader_fn, persist_dir)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, langchain\u001b[39m.\u001b[39mchains\u001b[39m.\u001b[39mbase\u001b[39m.\u001b[39mChain):\n\u001b[1;32m    621\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    622\u001b[0m         _UNSUPPORTED_MODEL_WARNING_MESSAGE,\n\u001b[1;32m    623\u001b[0m         \u001b[39mtype\u001b[39m(model)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m     model\u001b[39m.\u001b[39;49msave(model_data_path)\n\u001b[1;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[39mraise\u001b[39;00m mlflow\u001b[39m.\u001b[39mMlflowException\u001b[39m.\u001b[39minvalid_parameter_value(\n\u001b[1;32m    628\u001b[0m         _UNSUPPORTED_MODEL_ERROR_MESSAGE\u001b[39m.\u001b[39mformat(instance_type\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m(model)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    629\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.mlflow/lib/python3.9/site-packages/langchain/chains/conversational_retrieval/base.py:221\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain.save\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_chat_history:\n\u001b[1;32m    220\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mChain not saveable when `get_chat_history` is not None.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 221\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msave(file_path)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.mlflow/lib/python3.9/site-packages/langchain/chains/base.py:637\u001b[0m, in \u001b[0;36mChain.save\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Save the chain.\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \n\u001b[1;32m    625\u001b[0m \u001b[39mExpects `Chain._chain_type` property to be implemented and for memory to be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[39m        chain.save(file_path=\"path/chain.yaml\")\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 637\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSaving of memory is not yet supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    639\u001b[0m \u001b[39m# Fetch dictionary to save\u001b[39;00m\n\u001b[1;32m    640\u001b[0m chain_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdict()\n",
      "\u001b[0;31mValueError\u001b[0m: Saving of memory is not yet supported."
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "        chat_history = lambda h: h\n",
    "        logged_model = mlflow.langchain.log_model(\n",
    "            qa_api_version,\n",
    "            artifact_path=\"conversational_retrieval_qa_api_version\",\n",
    "            loader_fn=load_retriever,\n",
    "            persist_dir=persist_dir,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the model from MLFlow before running the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(logged_model.model_uri)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Query\": \"SELECT SUM(processing_time) AS total_time FROM hive_metastore.unum_askskan.events WHERE agent_type != 0 AND event_date >= '2023-04-01' AND event_date <= '2023-04-30'\",\n",
      "    \"Column\": \"total_time\",\n",
      "    \"Code\": \"import pandas as pd\\nfrom pyspark.sql import SparkSession\\n\\nspark = SparkSession.builder.appName('schema').getOrCreate()\\ndata = spark.read.csv('../askskan/data/original/sample_data.csv', header=True, inferSchema=True)\\ndata.createOrReplaceTempView('hive_metastore.unum_askskan.events')\\n\\nquery = 'SELECT SUM(processing_time) AS total_time FROM hive_metastore.unum_askskan.events WHERE agent_type != 0 AND event_date >= '2023-04-01' AND event_date <= '2023-04-30''\\nresult = spark.sql(query)\\n\\nspark.stop()\\n\",\n",
      "    \"Skan Bot\": \"The total time spent on process applications is #result#.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# result = qa({\"question\": question_1})\n",
    "# print(result['answer'])\n",
    "\n",
    "print(loaded_model.predict([{\"question\": question_1}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "Human: Total time spent on process application?\n",
      "AI: Doubt: Are you asking for the total time spent on all process applications combined or for a specific process application?\n",
      "Human: total time spent on all process applications combined\n",
      "AI: The total time spent on all process applications combined can be calculated by summing up the processing_time for all process applications. Let me generate the Spark SQL query to calculate it.\n",
      "Follow Up Input: Where is the Spark SQL query?\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n",
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a super smart code generator.\n",
      "Perform the following actions:\n",
      "\n",
      "1. Understand the input by human, schema and schema definitions each delimited by <>\n",
      "2. Based on the question, the schema and the schema definitions, first seek to clarify any ambiguities.\n",
      "    - Assume human does not know anything about the schema, so never ask details about the schema or schema         definitions or technical details in the clarification points.\n",
      "    - First try to find the answer to the ambiguities from the schema and schema definitions yourself.\n",
      "    - If the ambiguities still persist then: \n",
      "        1. Without mentioning about the schema or schema definitions only think of a list of super short bullets of             areas that need clarification in simple terms. \n",
      "        2. Then pick one clarification point, and wait for an answer from the human before moving to the next point.\n",
      "3. Never ask the human to explain the schema or schema definitions.\n",
      "4. Only human can answer these clarification points.\n",
      "5. Do not assume any schema attributes unless stated explicitly.\n",
      "6. Only after all the clarification points are answered:\n",
      "    - Use only columns in schema to generate a final Spark SQL query answering the original question. Refer to the         schema definitions only for more clarity about terms in the schema.\n",
      "    - Output computationally most efficient Spark SQL query.\n",
      "7. You can ask maximum 7 clarification points.\n",
      "9. I have a pandas csv table in data table delimited by <> that contains the data to be queried.\n",
      "    - Output a valid python code to execute the generated Spark SQL query on the data file ending with a print statement         showing the final answer in a friendly tone.\n",
      "10. Omit any explanations.\n",
      "\n",
      "Schema: <Name: event_id\n",
      "Description: Unique Identifier for a particular event.\n",
      "Sample_Value: 0186dcc060cb68888b8a088ec9c9332e\n",
      "\n",
      "Name: activity_id\n",
      "Description: Id assigned to the activity which describes a distinct event.\n",
      "Sample_Value: 3d4f6638f0658f635e9eb2d99c618b0f0b39c2818d8b3182174521328712f8d1\n",
      "\n",
      "Name: url\n",
      "Description: url of the active application in view or application being processed currently.\n",
      "Sample_Value: yatra.com/\n",
      "\n",
      "Name: parent_activity_id\n",
      "Description: List of Ids of all activities in the Abstraction Hierarchy which are ancestors of the current Activity .\n",
      "Sample_Value: [f387e55bd5281cfba5920223f853b5e9454f64eedf2b89786c671f35eec593bc, 44b4e13c1bafa59b9f9c33eaf82b7abd8239365e4bff679a6bdca5f09fbdbe0d, f40f0f2b3f510731c09b1df99804e107d16dfb83f71075d73dec25c0f3c6f207, 84aad2babf44ea02ab847ffd6ef4edd39171c6b1cd1b45a3ccbd5e8701c77d2b, 942c4e3fcb81f722a01c4c2008e017432ec33b41be435f8e1c6b0b9962789cef]>\n",
      "Schema definitions: <Event:A specific occurrence of an action performed by the user at a given time.\n",
      "Activity:Describes a distinct user event, includes both process and non-process events. Only process events can be named. eg. of process events - enter first name, enter address details etc.\n",
      "Abstraction Hierarchy:An application-specific organization of activities logically grouped together into a hierarchical tree structure.\n",
      "Activity Abstraction: Also referred to as task. It is a node in the Abstraction Hierarchy Tree, a chosen logical group of activities within the Abstraction Hierarchy.\n",
      "Activity Instance:A participant-specific occurance of consecutive sequence of events at a given time belonging to the same activity abstraction.\n",
      "Non actionable event:Keyboard or mouse events which does not create user events that can be named are called non-actionable event. eg. alphabets, numbers, mouse scroll, mouse click etc. Also referred as non-process event.\n",
      "Case Identifier:A case will have one more case identifiers which are configured by process owner. Each case identifier has a name and a corresponding value that uniquely identifies the case.\n",
      "Participant: A human that actively engages with the system by performing actions or contributing to the event's occurrence. They can also be referred to as a resource and user. \n",
      "Virtual Assistant: A bot sitting on the user's system that captures the events that observes the participant actions and captures them.\n",
      "\n",
      ">\n",
      "Data table: <../askskan/data/original/sample_data.csv>\n",
      "\n",
      "\n",
      "If there are clarification points and no Spark SQL query, ask the clarification question in this format:\n",
      "    Doubt: The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"Doubt\": {\"title\": \"Doubt\", \"description\": \"The clarification point asked by the bot\", \"type\": \"string\"}}, \"required\": [\"Doubt\"]}\n",
      "```\n",
      "\n",
      "After all the clarification points are answered by the human and there's a Spark SQL query generated, use this format:\n",
      "    The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"Query\": {\"title\": \"Query\", \"description\": \"The generated Spark SQL query\", \"type\": \"string\"}, \"Code\": {\"title\": \"Code\", \"description\": \"Python code generated to run the Spark SQL query\", \"type\": \"string\"}, \"Skan_Bot\": {\"title\": \"Skan Bot\", \"description\": \"The final answer printed by the python code in a friendly tone. Place the answer between ##\", \"type\": \"string\"}}, \"required\": [\"Query\", \"Code\", \"Skan_Bot\"]}\n",
      "```\n",
      "\n",
      "Current conversation:\n",
      "Human: Total time spent on process application?\n",
      "AI: Doubt: Are you asking for the total time spent on all process applications combined or for a specific process application?\n",
      "Human: total time spent on all process applications combined\n",
      "AI: The total time spent on all process applications combined can be calculated by summing up the processing_time for all process applications. Let me generate the Spark SQL query to calculate it.   \n",
      "Human: <Where can I find the Spark SQL query?>\n",
      "Skan Bot: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"Query\": {\"title\": \"Query\", \"description\": \"The generated Spark SQL query\", \"type\": \"string\"}, \"Code\": {\"title\": \"Code\", \"description\": \"Python code generated to run the Spark SQL query\", \"type\": \"string\"}, \"Skan_Bot\": {\"title\": \"Skan Bot\", \"description\": \"The final answer printed by the python code in a friendly tone. Place the answer between ##\", \"type\": \"string\"}}, \"required\": [\"Query\", \"Code\", \"Skan_Bot\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "result = qa({\"question\": \"Where is the Spark SQL query?\"})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "Human: Which is the most active time window during the day?\n",
      "AI: Doubt: Can you please clarify what you mean by \"active time window\"? Are you referring to the time of day with the highest active time?\n",
      "Human: Yes.\n",
      "AI: Doubt: Can you please clarify what you mean by \"highest level of activity\"? Are you referring to the time period with the highest total active time?\n",
      "Human: Yes.\n",
      "AI: Doubt: Can you please clarify the time range for the active time window? Are you looking for the most active time window within a specific day or across multiple days?\n",
      "Follow Up Input: Within a specific day.\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n",
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a super smart code generator.\n",
      "Perform the following actions:\n",
      "\n",
      "1. Understand the input by human, schema and schema definitions each delimited by <>\n",
      "2. Based on the question, the schema and the schema definitions, first seek to clarify any ambiguities.\n",
      "    - Assume human does not know anything about the schema, so never ask details about the schema or schema         definitions or technical details in the clarification points.\n",
      "    - First try to find the answer to the ambiguities from the schema and schema definitions yourself.\n",
      "    - If the ambiguities still persist then: \n",
      "        1. Without mentioning about the schema or schema definitions only think of a list of super short bullets of             areas that need clarification in simple terms. \n",
      "        2. Then pick one clarification point, and wait for an answer from the human before moving to the next point.\n",
      "3. Never ask the human to explain the schema or schema definitions.\n",
      "4. Only human can answer these clarification points.\n",
      "5. Do not assume any schema attributes unless stated explicitly.\n",
      "6. Only after all the clarification points are answered:\n",
      "    - Use only columns in schema to generate a final Spark SQL query answering the original question. Refer to the         schema definitions only for more clarity about terms in the schema.\n",
      "    - Output computationally most efficient Spark SQL query.\n",
      "7. You can ask maximum 7 clarification points.\n",
      "9. I have a pandas csv table in data table delimited by <> that contains the data to be queried.\n",
      "    - Output a valid python code to execute the generated Spark SQL query on the data file ending with a print statement         showing the final answer in a friendly tone.\n",
      "10. Omit any explanations.\n",
      "\n",
      "Schema: <Name: active_time\n",
      "Description: Active time measures the duration when a user is actively interacting with the computer by using the keyboard or mouse between two actions on the computer. It is calculated by dividing the time between the previous and current actions into one-minute intervals. If there is any keyboard or mouse activity within these intervals, that time is counted as part of the active time.\n",
      "Sample_Value: 0\n",
      "\n",
      "Name: processing_time\n",
      "Description: Total idle time below thresh hold-window + active_time (default thresh hold is 3 minutes). Processing time can also be referred to as active_time_in_view (This is different from active_time).\n",
      "Sample_Value: 0.72\n",
      "\n",
      "Name: wait_time\n",
      "Description: Total duration of idle time above the threshold-window (default threshold window is 3 minutes). Wait_time can also be referred to as inactive_time.\n",
      "Sample_Value: 1838.22\n",
      "\n",
      "Name: idle_time\n",
      "Description: Total Time where there is no keyboard or mouse activity within a minute window\n",
      "Sample_Value: 1838.22>\n",
      "Schema definitions: <Event:A specific occurrence of an action performed by the user at a given time.\n",
      "Activity:Describes a distinct user event, includes both process and non-process events. Only process events can be named. eg. of process events - enter first name, enter address details etc.\n",
      "Abstraction Hierarchy:An application-specific organization of activities logically grouped together into a hierarchical tree structure.\n",
      "Activity Abstraction: Also referred to as task. It is a node in the Abstraction Hierarchy Tree, a chosen logical group of activities within the Abstraction Hierarchy.\n",
      "Activity Instance:A participant-specific occurance of consecutive sequence of events at a given time belonging to the same activity abstraction.\n",
      "Non actionable event:Keyboard or mouse events which does not create user events that can be named are called non-actionable event. eg. alphabets, numbers, mouse scroll, mouse click etc. Also referred as non-process event.\n",
      "Case Identifier:A case will have one more case identifiers which are configured by process owner. Each case identifier has a name and a corresponding value that uniquely identifies the case.\n",
      "Participant: A human that actively engages with the system by performing actions or contributing to the event's occurrence. They can also be referred to as a resource and user. \n",
      "Virtual Assistant: A bot sitting on the user's system that captures the events that observes the participant actions and captures them.\n",
      "\n",
      ">\n",
      "Data table: <../askskan/data/original/sample_data.csv>\n",
      "\n",
      "\n",
      "If there are clarification points and no Spark SQL query, ask the clarification question in this format:\n",
      "    Doubt: <clarification point>\n",
      "\n",
      "After all the clarification points are answered by the human and there's a Spark SQL query generated, use this format:\n",
      "    Output: The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"Query\": {\"title\": \"Query\", \"description\": \"The generated Spark SQL query\", \"type\": \"string\"}, \"Code\": {\"title\": \"Code\", \"description\": \"Python code generated to run the Spark SQL query\", \"type\": \"string\"}, \"Skan_Bot\": {\"title\": \"Skan Bot\", \"description\": \"The final answer printed by the python code in a friendly tone. Place the answer between ##\", \"type\": \"string\"}}, \"required\": [\"Query\", \"Code\", \"Skan_Bot\"]}\n",
      "```\n",
      "\n",
      "Current conversation:\n",
      "Human: Which is the most active time window during the day?\n",
      "AI: Doubt: Can you please clarify what you mean by \"active time window\"? Are you referring to the time of day with the highest active time?\n",
      "Human: Yes.\n",
      "AI: Doubt: Can you please clarify what you mean by \"highest level of activity\"? Are you referring to the time period with the highest total active time?\n",
      "Human: Yes.\n",
      "AI: Doubt: Can you please clarify the time range for the active time window? Are you looking for the most active time window within a specific day or across multiple days?   \n",
      "Human: <What is the most active time window within a specific day?>\n",
      "Skan Bot: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{\n",
      "  \"Query\": \"SELECT MAX(active_time) AS max_active_time, CONCAT(HOUR(event_time), ':', MINUTE(event_time)) AS active_time_window FROM data_table GROUP BY CONCAT(HOUR(event_time), ':', MINUTE(event_time)) ORDER BY max_active_time DESC LIMIT 1\",\n",
      "  \"Code\": \"import pandas as pd\\nfrom pyspark.sql import SparkSession\\n\\n# Create Spark session\\nspark = SparkSession.builder.appName('ActiveTimeWindow').getOrCreate()\\n\\n# Read CSV file\\ndf = spark.read.format('csv').option('header', 'true').load('../askskan/data/original/sample_data.csv')\\n\\n# Register DataFrame as a temporary table\\ndf.createOrReplaceTempView('data_table')\\n\\n# Generate Spark SQL query\\nquery = \\\"SELECT MAX(active_time) AS max_active_time, CONCAT(HOUR(event_time), ':', MINUTE(event_time)) AS active_time_window FROM data_table GROUP BY CONCAT(HOUR(event_time), ':', MINUTE(event_time)) ORDER BY max_active_time DESC LIMIT 1\\\"\\n\\n# Execute query\\nresult = spark.sql(query)\\n\\n# Convert result to Pandas DataFrame\\nresult_df = result.toPandas()\\n\\n# Print the final answer\\nprint('The most active time window during the day is', result_df['active_time_window'][0], 'with a total active time of', result_df['max_active_time'][0], 'minutes.')\",\n",
      "  \"Skan_Bot\": \"The most active time window during the day is <active_time_window> with a total active time of <max_active_time> minutes.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = qa({\"question\": \"Within a specific day.\"})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "# Create Spark session\n",
      "spark = SparkSession.builder.appName('ActiveTimeWindow').getOrCreate()\n",
      "\n",
      "# Read CSV file\n",
      "df = spark.read.format('csv').option('header', 'true').load('../askskan/data/original/sample_data.csv')\n",
      "\n",
      "# Register DataFrame as a temporary table\n",
      "df.createOrReplaceTempView('data_table')\n",
      "\n",
      "# Generate Spark SQL query\n",
      "query = \"SELECT MAX(active_time) AS max_active_time, CONCAT(HOUR(event_time), ':', MINUTE(event_time)) AS active_time_window FROM data_table GROUP BY CONCAT(HOUR(event_time), ':', MINUTE(event_time)) ORDER BY max_active_time DESC LIMIT 1\"\n",
      "\n",
      "# Execute query\n",
      "result = spark.sql(query)\n",
      "\n",
      "# Convert result to Pandas DataFrame\n",
      "result_df = result.toPandas()\n",
      "\n",
      "# Print the final answer\n",
      "print('The most active time window during the day is', result_df['active_time_window'][0], 'with a total active time of', result_df['max_active_time'][0], 'minutes.')\n"
     ]
    }
   ],
   "source": [
    "code = parser.parse(result['answer']).Code\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import RetryWithErrorOutputParser\n",
    "retry_parser = RetryWithErrorOutputParser.from_llm(\n",
    "    parser=parser, llm=OpenAI(temperature=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PromptTemplate' object has no attribute 'to_string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/langchain/output_parsers/pydantic.py:25\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     24\u001b[0m     json_str \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup()\n\u001b[0;32m---> 25\u001b[0m json_object \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(json_str, strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     26\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpydantic_object\u001b[39m.\u001b[39mparse_obj(json_object)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m     kw[\u001b[39m'\u001b[39m\u001b[39mparse_constant\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m parse_constant\n\u001b[0;32m--> 359\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\u001b[39m.\u001b[39;49mdecode(s)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/langchain/output_parsers/retry.py:109\u001b[0m, in \u001b[0;36mRetryWithErrorOutputParser.parse_with_prompt\u001b[0;34m(self, completion, prompt_value)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     parsed_completion \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparser\u001b[39m.\u001b[39;49mparse(completion)\n\u001b[1;32m    110\u001b[0m \u001b[39mexcept\u001b[39;00m OutputParserException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/langchain/output_parsers/pydantic.py:31\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     30\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to parse \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m from completion \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m. Got: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[39mraise\u001b[39;00m OutputParserException(msg)\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Failed to parse FormatOutput from completion import pandas as pd\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\ndata = pd.read_csv('../askskan/data/original/sample_data.csv')\ndata_table = spark.createDataFrame(data)\ndata_table.createOrReplaceTempView('data_table')\n\nquery = \"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\"\nresult = spark.sql(query)\nresult.show()\n. Got: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[217], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m retry_parser\u001b[39m.\u001b[39;49mparse_with_prompt(code, new_prompt)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/langchain/output_parsers/retry.py:112\u001b[0m, in \u001b[0;36mRetryWithErrorOutputParser.parse_with_prompt\u001b[0;34m(self, completion, prompt_value)\u001b[0m\n\u001b[1;32m    109\u001b[0m     parsed_completion \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparser\u001b[39m.\u001b[39mparse(completion)\n\u001b[1;32m    110\u001b[0m \u001b[39mexcept\u001b[39;00m OutputParserException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    111\u001b[0m     new_completion \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry_chain\u001b[39m.\u001b[39mrun(\n\u001b[0;32m--> 112\u001b[0m         prompt\u001b[39m=\u001b[39mprompt_value\u001b[39m.\u001b[39;49mto_string(), completion\u001b[39m=\u001b[39mcompletion, error\u001b[39m=\u001b[39m\u001b[39mrepr\u001b[39m(e)\n\u001b[1;32m    113\u001b[0m     )\n\u001b[1;32m    114\u001b[0m     parsed_completion \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparser\u001b[39m.\u001b[39mparse(new_completion)\n\u001b[1;32m    116\u001b[0m \u001b[39mreturn\u001b[39;00m parsed_completion\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PromptTemplate' object has no attribute 'to_string'"
     ]
    }
   ],
   "source": [
    "retry_parser.parse_with_prompt(code, new_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "Human: Which is the most used application?\n",
      "AI: Doubt: Can you please clarify what you mean by \"most used application\"? Are you asking for the application with the highest active time or the application that was switched to the most frequently?\n",
      "Human: Appliction with highest active time.\n",
      "AI: Doubt: Are you asking for the application with the highest active time in general or for a specific time period?\n",
      "Human: In general\n",
      "AI: Output: The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "```\n",
      "{\n",
      "  \"Query\": \"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\",\n",
      "  \"Code\": \"import pandas as pd\\nfrom pyspark.sql import SparkSession\\n\\nspark = SparkSession.builder.getOrCreate()\\ndata = pd.read_csv('../askskan/data/original/sample_data.csv')\\ndata_table = spark.createDataFrame(data)\\ndata_table.createOrReplaceTempView('data_table')\\n\\nquery = \\\"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\\\"\\nresult = spark.sql(query)\\nresult.show()\\n\",\n",
      "  \"Skan_Bot\": \"The application with the highest active time is <application name> with an active time of <highest active time>.\"\n",
      "}\n",
      "```\n",
      "Human: Which is the most active time window during the day?\n",
      "AI: Doubt: Can you please clarify what you mean by \"active time window\"? Are you asking for the time period during the day with the highest active time?\n",
      "Human: Yes\n",
      "AI: Yes, I am asking for the time period during the day with the highest active time.\n",
      "Human: Yes, the time period during the day with the highest active time.\n",
      "AI: Doubt: Can you please specify the format in which you want the time period during the day to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in between 2 different times. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output format to be in between 2 different times. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in another format. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Follow Up Input: I want the output to be in another format.\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n",
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a super smart code generator.\n",
      "Perform the following actions:\n",
      "\n",
      "1. Understand the input by human, schema and schema definitions each delimited by <>\n",
      "2. Based on the question, the schema and the schema definitions, first seek to clarify any ambiguities.\n",
      "    - Assume human does not know anything about the schema, so never ask details about the schema or schema         definitions or technical details in the clarification points.\n",
      "    - First try to find the answer to the ambiguities from the schema and schema definitions yourself.\n",
      "    - If the ambiguities still persist then: \n",
      "        1. Without mentioning about the schema or schema definitions only think of a list of super short bullets of             areas that need clarification in simple terms. \n",
      "        2. Then pick one clarification point, and wait for an answer from the human before moving to the next point.\n",
      "3. Never ask the human to explain the schema or schema definitions.\n",
      "4. Only human can answer these clarification points.\n",
      "5. Do not assume any schema attributes unless stated explicitly.\n",
      "6. Only after all the clarification points are answered:\n",
      "    - Use only columns in schema to generate a final Spark SQL query answering the original question. Refer to the         schema definitions only for more clarity about terms in the schema.\n",
      "    - Output computationally most efficient Spark SQL query.\n",
      "7. You can ask maximum 7 clarification points.\n",
      "9. I have a pandas csv table in data table delimited by <> that contains the data to be queried.\n",
      "    - Output a valid python code to execute the generated Spark SQL query on the data file ending with a print statement         showing the final answer in a friendly tone.\n",
      "10. Omit any explanations.\n",
      "\n",
      "Schema: <Name: active_time\n",
      "Description: Active time measures the duration when a user is actively interacting with the computer by using the keyboard or mouse between two actions on the computer. It is calculated by dividing the time between the previous and current actions into one-minute intervals. If there is any keyboard or mouse activity within these intervals, that time is counted as part of the active time.\n",
      "Sample_Value: 0\n",
      "\n",
      "Name: processing_time\n",
      "Description: Total idle time below thresh hold-window + active_time (default thresh hold is 3 minutes). Processing time can also be referred to as active_time_in_view (This is different from active_time).\n",
      "Sample_Value: 0.72\n",
      "\n",
      "Name: idle_time\n",
      "Description: Total Time where there is no keyboard or mouse activity within a minute window\n",
      "Sample_Value: 1838.22\n",
      "\n",
      "Name: activity_instance_original_end_time\n",
      "Description: End time is the time of the last event in the Activity Instance. The event can be a user event or non actionable event. Last Event may be the current user event or a subsequest user event or non actionable event.\n",
      "Sample_Value: 2023-04-03 14:55:13.907000>\n",
      "Schema definitions: <Event:A specific occurrence of an action performed by the user at a given time.\n",
      "Activity:Describes a distinct user event, includes both process and non-process events. Only process events can be named. eg. of process events - enter first name, enter address details etc.\n",
      "Abstraction Hierarchy:An application-specific organization of activities logically grouped together into a hierarchical tree structure.\n",
      "Activity Abstraction: Also referred to as task. It is a node in the Abstraction Hierarchy Tree, a chosen logical group of activities within the Abstraction Hierarchy.\n",
      "Activity Instance:A participant-specific occurance of consecutive sequence of events at a given time belonging to the same activity abstraction.\n",
      "Non actionable event:Keyboard or mouse events which does not create user events that can be named are called non-actionable event. eg. alphabets, numbers, mouse scroll, mouse click etc. Also referred as non-process event.\n",
      "Case Identifier:A case will have one more case identifiers which are configured by process owner. Each case identifier has a name and a corresponding value that uniquely identifies the case.\n",
      "Participant: A human that actively engages with the system by performing actions or contributing to the event's occurrence. They can also be referred to as a resource and user. \n",
      "Virtual Assistant: A bot sitting on the user's system that captures the events that observes the participant actions and captures them.\n",
      "\n",
      ">\n",
      "Data table: <../askskan/data/original/sample_data.csv>\n",
      "\n",
      "\n",
      "If there are clarification points and no Spark SQL query, ask the clarification question in this format:\n",
      "    Doubt: <clarification point>\n",
      "\n",
      "After all the clarification points are answered by the human and there's a Spark SQL query generated, use this format:\n",
      "    Output: The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"Query\": {\"title\": \"Query\", \"description\": \"The generated Spark SQL query\", \"type\": \"string\"}, \"Code\": {\"title\": \"Code\", \"description\": \"Python code generated to run the Spark SQL query\", \"type\": \"string\"}, \"Skan_Bot\": {\"title\": \"Skan Bot\", \"description\": \"The final answer printed by the python code in a friendly tone. Place the answer between ##\", \"type\": \"string\"}}, \"required\": [\"Query\", \"Code\", \"Skan_Bot\"]}\n",
      "```\n",
      "\n",
      "Current conversation:\n",
      "Human: Which is the most used application?\n",
      "AI: Doubt: Can you please clarify what you mean by \"most used application\"? Are you asking for the application with the highest active time or the application that was switched to the most frequently?\n",
      "Human: Appliction with highest active time.\n",
      "AI: Doubt: Are you asking for the application with the highest active time in general or for a specific time period?\n",
      "Human: In general\n",
      "AI: Output: The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "```\n",
      "{\n",
      "  \"Query\": \"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\",\n",
      "  \"Code\": \"import pandas as pd\\nfrom pyspark.sql import SparkSession\\n\\nspark = SparkSession.builder.getOrCreate()\\ndata = pd.read_csv('../askskan/data/original/sample_data.csv')\\ndata_table = spark.createDataFrame(data)\\ndata_table.createOrReplaceTempView('data_table')\\n\\nquery = \\\"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\\\"\\nresult = spark.sql(query)\\nresult.show()\\n\",\n",
      "  \"Skan_Bot\": \"The application with the highest active time is <application name> with an active time of <highest active time>.\"\n",
      "}\n",
      "```\n",
      "Human: Which is the most active time window during the day?\n",
      "AI: Doubt: Can you please clarify what you mean by \"active time window\"? Are you asking for the time period during the day with the highest active time?\n",
      "Human: Yes\n",
      "AI: Yes, I am asking for the time period during the day with the highest active time.\n",
      "Human: Yes, the time period during the day with the highest active time.\n",
      "AI: Doubt: Can you please specify the format in which you want the time period during the day to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in between 2 different times. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output format to be in between 2 different times. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in another format. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?   \n",
      "Human: <In what format would you like the time period during the day with the highest active time to be displayed?>\n",
      "Skan Bot: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n"
     ]
    }
   ],
   "source": [
    "result = qa({\"question\": \"I want the output to be in another format.\"})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "Human: Which is the most used application?\n",
      "AI: Doubt: Can you please clarify what you mean by \"most used application\"? Are you asking for the application with the highest active time or the application that was switched to the most frequently?\n",
      "Human: Appliction with highest active time.\n",
      "AI: Doubt: Are you asking for the application with the highest active time in general or for a specific time period?\n",
      "Human: In general\n",
      "AI: Output: The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "```\n",
      "{\n",
      "  \"Query\": \"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\",\n",
      "  \"Code\": \"import pandas as pd\\nfrom pyspark.sql import SparkSession\\n\\nspark = SparkSession.builder.getOrCreate()\\ndata = pd.read_csv('../askskan/data/original/sample_data.csv')\\ndata_table = spark.createDataFrame(data)\\ndata_table.createOrReplaceTempView('data_table')\\n\\nquery = \\\"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\\\"\\nresult = spark.sql(query)\\nresult.show()\\n\",\n",
      "  \"Skan_Bot\": \"The application with the highest active time is <application name> with an active time of <highest active time>.\"\n",
      "}\n",
      "```\n",
      "Human: Which is the most active time window during the day?\n",
      "AI: Doubt: Can you please clarify what you mean by \"active time window\"? Are you asking for the time period during the day with the highest active time?\n",
      "Human: Yes\n",
      "AI: Yes, I am asking for the time period during the day with the highest active time.\n",
      "Human: Yes, the time period during the day with the highest active time.\n",
      "AI: Doubt: Can you please specify the format in which you want the time period during the day to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in between 2 different times. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output format to be in between 2 different times. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in another format. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in another format.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Follow Up Input: I want the output to be in hours and minutes (HH:MM).\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n",
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a super smart code generator.\n",
      "Perform the following actions:\n",
      "\n",
      "1. Understand the input by human, schema and schema definitions each delimited by <>\n",
      "2. Based on the question, the schema and the schema definitions, first seek to clarify any ambiguities.\n",
      "    - Assume human does not know anything about the schema, so never ask details about the schema or schema         definitions or technical details in the clarification points.\n",
      "    - First try to find the answer to the ambiguities from the schema and schema definitions yourself.\n",
      "    - If the ambiguities still persist then: \n",
      "        1. Without mentioning about the schema or schema definitions only think of a list of super short bullets of             areas that need clarification in simple terms. \n",
      "        2. Then pick one clarification point, and wait for an answer from the human before moving to the next point.\n",
      "3. Never ask the human to explain the schema or schema definitions.\n",
      "4. Only human can answer these clarification points.\n",
      "5. Do not assume any schema attributes unless stated explicitly.\n",
      "6. Only after all the clarification points are answered:\n",
      "    - Use only columns in schema to generate a final Spark SQL query answering the original question. Refer to the         schema definitions only for more clarity about terms in the schema.\n",
      "    - Output computationally most efficient Spark SQL query.\n",
      "7. You can ask maximum 7 clarification points.\n",
      "9. I have a pandas csv table in data table delimited by <> that contains the data to be queried.\n",
      "    - Output a valid python code to execute the generated Spark SQL query on the data file ending with a print statement         showing the final answer in a friendly tone.\n",
      "10. Omit any explanations.\n",
      "\n",
      "Schema: <Name: active_time\n",
      "Description: Active time measures the duration when a user is actively interacting with the computer by using the keyboard or mouse between two actions on the computer. It is calculated by dividing the time between the previous and current actions into one-minute intervals. If there is any keyboard or mouse activity within these intervals, that time is counted as part of the active time.\n",
      "Sample_Value: 0\n",
      "\n",
      "Name: processing_time\n",
      "Description: Total idle time below thresh hold-window + active_time (default thresh hold is 3 minutes). Processing time can also be referred to as active_time_in_view (This is different from active_time).\n",
      "Sample_Value: 0.72\n",
      "\n",
      "Name: idle_time\n",
      "Description: Total Time where there is no keyboard or mouse activity within a minute window\n",
      "Sample_Value: 1838.22\n",
      "\n",
      "Name: activity_instance_original_end_time\n",
      "Description: End time is the time of the last event in the Activity Instance. The event can be a user event or non actionable event. Last Event may be the current user event or a subsequest user event or non actionable event.\n",
      "Sample_Value: 2023-04-03 14:55:13.907000>\n",
      "Schema definitions: <Event:A specific occurrence of an action performed by the user at a given time.\n",
      "Activity:Describes a distinct user event, includes both process and non-process events. Only process events can be named. eg. of process events - enter first name, enter address details etc.\n",
      "Abstraction Hierarchy:An application-specific organization of activities logically grouped together into a hierarchical tree structure.\n",
      "Activity Abstraction: Also referred to as task. It is a node in the Abstraction Hierarchy Tree, a chosen logical group of activities within the Abstraction Hierarchy.\n",
      "Activity Instance:A participant-specific occurance of consecutive sequence of events at a given time belonging to the same activity abstraction.\n",
      "Non actionable event:Keyboard or mouse events which does not create user events that can be named are called non-actionable event. eg. alphabets, numbers, mouse scroll, mouse click etc. Also referred as non-process event.\n",
      "Case Identifier:A case will have one more case identifiers which are configured by process owner. Each case identifier has a name and a corresponding value that uniquely identifies the case.\n",
      "Participant: A human that actively engages with the system by performing actions or contributing to the event's occurrence. They can also be referred to as a resource and user. \n",
      "Virtual Assistant: A bot sitting on the user's system that captures the events that observes the participant actions and captures them.\n",
      "\n",
      ">\n",
      "Data table: <../askskan/data/original/sample_data.csv>\n",
      "\n",
      "\n",
      "If there are clarification points and no Spark SQL query, ask the clarification question in this format:\n",
      "    Doubt: <clarification point>\n",
      "\n",
      "After all the clarification points are answered by the human and there's a Spark SQL query generated, use this format:\n",
      "    Output: The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"Query\": {\"title\": \"Query\", \"description\": \"The generated Spark SQL query\", \"type\": \"string\"}, \"Code\": {\"title\": \"Code\", \"description\": \"Python code generated to run the Spark SQL query\", \"type\": \"string\"}, \"Skan_Bot\": {\"title\": \"Skan Bot\", \"description\": \"The final answer printed by the python code in a friendly tone. Place the answer between ##\", \"type\": \"string\"}}, \"required\": [\"Query\", \"Code\", \"Skan_Bot\"]}\n",
      "```\n",
      "\n",
      "Current conversation:\n",
      "Human: Which is the most used application?\n",
      "AI: Doubt: Can you please clarify what you mean by \"most used application\"? Are you asking for the application with the highest active time or the application that was switched to the most frequently?\n",
      "Human: Appliction with highest active time.\n",
      "AI: Doubt: Are you asking for the application with the highest active time in general or for a specific time period?\n",
      "Human: In general\n",
      "AI: Output: The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "```\n",
      "{\n",
      "  \"Query\": \"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\",\n",
      "  \"Code\": \"import pandas as pd\\nfrom pyspark.sql import SparkSession\\n\\nspark = SparkSession.builder.getOrCreate()\\ndata = pd.read_csv('../askskan/data/original/sample_data.csv')\\ndata_table = spark.createDataFrame(data)\\ndata_table.createOrReplaceTempView('data_table')\\n\\nquery = \\\"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\\\"\\nresult = spark.sql(query)\\nresult.show()\\n\",\n",
      "  \"Skan_Bot\": \"The application with the highest active time is <application name> with an active time of <highest active time>.\"\n",
      "}\n",
      "```\n",
      "Human: Which is the most active time window during the day?\n",
      "AI: Doubt: Can you please clarify what you mean by \"active time window\"? Are you asking for the time period during the day with the highest active time?\n",
      "Human: Yes\n",
      "AI: Yes, I am asking for the time period during the day with the highest active time.\n",
      "Human: Yes, the time period during the day with the highest active time.\n",
      "AI: Doubt: Can you please specify the format in which you want the time period during the day to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in between 2 different times. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output format to be in between 2 different times. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in another format. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in another format.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?   \n",
      "Human: <In what format would you like the time period during the day with the highest active time to be displayed?>\n",
      "Skan Bot: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n"
     ]
    }
   ],
   "source": [
    "result = qa({\"question\": \"I want the output to be in hours and minutes (HH:MM).\"})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "spark = SparkSession.builder.getOrCreate()\n",
      "data = pd.read_csv('../askskan/data/original/sample_data.csv')\n",
      "data_table = spark.createDataFrame(data)\n",
      "data_table.createOrReplaceTempView('data_table')\n",
      "\n",
      "query = \"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\"\n",
      "result = spark.sql(query)\n",
      "result.show()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code = parser.parse(result['answer']).Code\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = qa({\"question\": \"processing_time field represents the time taken by the total time taken by a participant for all their tasks\"})\n",
    "# print(result['answer'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the output string to look like JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = result['answer']\n",
    "\n",
    "# Find and remove the first occurrence of \"Query\" in the string\n",
    "stripped_string = string.replace('Output:', '', 1).strip()\n",
    "\n",
    "print(stripped_string)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT SUM(processing_time) FROM data_table WHERE agent_type = 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_string = result['answer'] # stripped_string\n",
    "\n",
    "# Remove invalid escape sequences from the JSON string\n",
    "#json_string = json_string.encode('utf-8').decode('unicode_escape')\n",
    "\n",
    "# Convert the modified JSON string to a JSON object\n",
    "json_object = json.loads(json_string, strict=False)\n",
    "\n",
    "# Extract the \"Query\" field\n",
    "query = json_object['Query']\n",
    "\n",
    "# Print the extracted query\n",
    "print(query)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the Python code and answer string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"\n",
    "{\"Query\": [\"SELECT SUM(processing_time) AS total_time FROM data_table WHERE agent_type != 0\"],\n",
    "\"Code\": [\"import pandas as pd\\nfrom pyspark.sql import SparkSession\\n\\nspark = \\\n",
    "    SparkSession.builder.appName('schema').getOrCreate()\\ndata = spark.read.csv('data/sample/data0.csv', \\\n",
    "        header=True, inferSchema=True)\\ndata.createOrReplaceTempView('data_table')\\n\\nquery = \\\n",
    "            \"SELECT SUM(processing_time) AS total_time FROM data_table WHERE agent_type != 0\"\\nresult = \\\n",
    "                spark.sql(query)\\nresult.show()\\nprint('The total time spent on process applications is:', \\\n",
    "                    result.collect()[0][0])\"],\n",
    "\"Skan Bot\": [\"The total time spent on process applications is: #total_time#.\"],\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = \"\"\"\n",
    "{\"Code\": \"import pandas as pd\\nfrom pyspark.sql import SparkSession\\n\\nspark = \\\n",
    "SparkSession.builder.appName('schema').getOrCreate()\\ndata = spark.read.csv('data/sample/data0.csv', \\\n",
    "header=True, inferSchema=True)\\ndata.createOrReplaceTempView('data_table')\\n\\nquery = \\\n",
    "'SELECT SUM(processing_time) AS total_time FROM data_table WHERE agent_type != 0'\\nresult = \\\n",
    "spark.sql(query)\\nresult.show()\\nprint('The total time spent on process applications is:', \\\n",
    "result.collect()[0][0])\"\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "spark = SparkSession.builder.appName('schema').getOrCreate()\n",
      "data = spark.read.csv('data/sample/data0.csv', header=True, inferSchema=True)\n",
      "data.createOrReplaceTempView('data_table')\n",
      "\n",
      "query = 'SELECT SUM(processing_time) AS total_time FROM data_table WHERE agent_type != 0'\n",
      "result = spark.sql(query)\n",
      "result.show()\n",
      "print('The total time spent on process applications is:', result.collect()[0][0])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_string = string1 # result['answer']\n",
    "\n",
    "# Remove invalid escape sequences from the JSON string\n",
    "json_string = json_string.encode('utf-8').decode('unicode_escape')\n",
    "\n",
    "# Find and remove the first occurrence of \"Query\" in the JSON string\n",
    "#json_string = json_string.replace('Query', '', 1).strip()\n",
    "\n",
    "# Convert the modified JSON string to a JSON object\n",
    "json_object = json.loads(json_string, strict=False)\n",
    "\n",
    "# Extract the \"Query\" field\n",
    "code = json_object['Code']\n",
    "#answer = json_object['Skan Bot']\n",
    "\n",
    "# Print the extracted query\n",
    "print(code)\n",
    "#print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    exec(code)\n",
    "except Exception as e:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import sum\n",
    "\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Load the 'data' table into a DataFrame\n",
    "    data = spark.table('data')\n",
    "\n",
    "    # Apply the filter condition\n",
    "    filtered_data = data.filter(data.agent_type != 0)\n",
    "\n",
    "    # Calculate the sum of processing_time\n",
    "    result = filtered_data.select(sum('processing_time').alias('total_time'))\n",
    "\n",
    "    # Show the result\n",
    "    result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "#spark = SparkSession.builder.appName('Query').getOrCreate()\n",
    "\n",
    "# conf = SparkConf().setAppName(\"YourAppName\").setMaster(\"local\").set(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "# spark = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.appName(\"YourAppName\").master(\"local\").config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()\n",
    "\n",
    "data = spark.read.format('csv').option('header', 'true').load('../data/sample/data0.csv')\n",
    "data.createOrReplaceTempView('data0')\n",
    "\n",
    "query = 'SELECT AVG(processing_time) AS avg_processing_time FROM data'\n",
    "avg_processing_time = spark.sql(query).collect()[0]['avg_processing_time']\n",
    "\n",
    "print('The longest average processing time overall is: {} seconds.'.format(avg_processing_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Query').getOrCreate()\n",
    "data = spark.read.format('csv').option('header', 'true').load('../data/sample/data0.csv')\n",
    "data.createOrReplaceTempView('data0')\n",
    "\n",
    "query = 'SELECT participant_id, AVG(processing_time) AS avg_processing_time \\\n",
    "FROM data0 GROUP BY participant_id \\\n",
    "ORDER BY avg_processing_time \\\n",
    "DESC LIMIT 1'\n",
    "\n",
    "result_df = spark.sql(query).toPandas()\n",
    "print('The participant with the longest average processing time is:', result_df['participant_id'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ServiceRequestError",
     "evalue": "<urllib3.connection.HTTPSConnection object at 0x140512820>: Failed to resolve 'model-training' ([Errno 8] nodename nor servname provided, or not known)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServiceRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 31\u001b[0m\n\u001b[1;32m     21\u001b[0m blob_client \u001b[39m=\u001b[39m BlobClient(\n\u001b[1;32m     22\u001b[0m     account_url\u001b[39m=\u001b[39maccount_url, \n\u001b[1;32m     23\u001b[0m     container_name\u001b[39m=\u001b[39mcontainer_name, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     max_chunk_get_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m\u001b[39m*\u001b[39m\u001b[39m1024\u001b[39m\u001b[39m*\u001b[39m\u001b[39m4\u001b[39m \u001b[39m# 4 MiB\u001b[39;00m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../askskan/data/original/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mevents.csv\u001b[39m\u001b[39m'\u001b[39m), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m sample_blob:\n\u001b[0;32m---> 31\u001b[0m     download_stream \u001b[39m=\u001b[39m blob_client\u001b[39m.\u001b[39;49mdownload_blob(max_concurrency\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     32\u001b[0m     sample_blob\u001b[39m.\u001b[39mwrite(download_stream\u001b[39m.\u001b[39mreadall())\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/tracing/decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     78\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/storage/blob/_blob_client.py:914\u001b[0m, in \u001b[0;36mBlobClient.download_blob\u001b[0;34m(self, offset, length, encoding, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Downloads a blob to the StorageStreamDownloader. The readall() method must\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[39mbe used to read all the content or readinto() must be used to download the blob into\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[39ma stream. Using chunks() returns an iterator which allows the user to iterate over the content in chunks.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[39m        :caption: Download a blob.\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    909\u001b[0m options \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_blob_options(\n\u001b[1;32m    910\u001b[0m     offset\u001b[39m=\u001b[39moffset,\n\u001b[1;32m    911\u001b[0m     length\u001b[39m=\u001b[39mlength,\n\u001b[1;32m    912\u001b[0m     encoding\u001b[39m=\u001b[39mencoding,\n\u001b[1;32m    913\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 914\u001b[0m \u001b[39mreturn\u001b[39;00m StorageStreamDownloader(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/storage/blob/_download.py:366\u001b[0m, in \u001b[0;36mStorageStreamDownloader.__init__\u001b[0;34m(self, clients, config, start_range, end_range, validate_content, encryption_options, max_concurrency, name, container, encoding, download_cls, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m     initial_request_end \u001b[39m=\u001b[39m initial_request_start \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_first_get_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    358\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_range, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_offset \u001b[39m=\u001b[39m process_range_and_offset(\n\u001b[1;32m    359\u001b[0m     initial_request_start,\n\u001b[1;32m    360\u001b[0m     initial_request_end,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encryption_data\n\u001b[1;32m    364\u001b[0m )\n\u001b[0;32m--> 366\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initial_request()\n\u001b[1;32m    367\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproperties \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response\u001b[39m.\u001b[39mproperties\n\u001b[1;32m    368\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproperties\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/storage/blob/_download.py:418\u001b[0m, in \u001b[0;36mStorageStreamDownloader._initial_request\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39mwhile\u001b[39;00m retry_active:\n\u001b[1;32m    417\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 418\u001b[0m         location_mode, response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_clients\u001b[39m.\u001b[39;49mblob\u001b[39m.\u001b[39;49mdownload(\n\u001b[1;32m    419\u001b[0m             \u001b[39mrange\u001b[39;49m\u001b[39m=\u001b[39;49mrange_header,\n\u001b[1;32m    420\u001b[0m             range_get_content_md5\u001b[39m=\u001b[39;49mrange_validation,\n\u001b[1;32m    421\u001b[0m             validate_content\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_content,\n\u001b[1;32m    422\u001b[0m             data_stream_total\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    423\u001b[0m             download_stream_current\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m    424\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request_options\n\u001b[1;32m    425\u001b[0m         )\n\u001b[1;32m    427\u001b[0m         \u001b[39m# Check the location we read from to ensure we use the same one\u001b[39;00m\n\u001b[1;32m    428\u001b[0m         \u001b[39m# for subsequent requests.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_location_mode \u001b[39m=\u001b[39m location_mode\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/tracing/decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     78\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/storage/blob/_generated/operations/_blob_operations.py:1592\u001b[0m, in \u001b[0;36mBlobOperations.download\u001b[0;34m(self, snapshot, version_id, timeout, range, range_get_content_md5, range_get_content_crc64, request_id_parameter, lease_access_conditions, cpk_info, modified_access_conditions, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m request \u001b[39m=\u001b[39m _convert_request(request)\n\u001b[1;32m   1590\u001b[0m request\u001b[39m.\u001b[39murl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mformat_url(request\u001b[39m.\u001b[39murl)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m pipeline_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49m_pipeline\u001b[39m.\u001b[39;49mrun(  \u001b[39m# type: ignore # pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m   1593\u001b[0m     request, stream\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m   1594\u001b[0m )\n\u001b[1;32m   1596\u001b[0m response \u001b[39m=\u001b[39m pipeline_response\u001b[39m.\u001b[39mhttp_response\n\u001b[1;32m   1598\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m200\u001b[39m, \u001b[39m206\u001b[39m]:\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:202\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m pipeline_request: PipelineRequest[HTTPRequestType] \u001b[39m=\u001b[39m PipelineRequest(request, context)\n\u001b[1;32m    201\u001b[0m first_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl_policies[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl_policies \u001b[39melse\u001b[39;00m _TransportRunner(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transport)\n\u001b[0;32m--> 202\u001b[0m \u001b[39mreturn\u001b[39;00m first_node\u001b[39m.\u001b[39;49msend(pipeline_request)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "    \u001b[0;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 70 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/policies/_redirect.py:156\u001b[0m, in \u001b[0;36mRedirectPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    154\u001b[0m redirect_settings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfigure_redirects(request\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39moptions)\n\u001b[1;32m    155\u001b[0m \u001b[39mwhile\u001b[39;00m retryable:\n\u001b[0;32m--> 156\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m    157\u001b[0m     redirect_location \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_redirect_location(response)\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m redirect_location \u001b[39mand\u001b[39;00m redirect_settings[\u001b[39m\"\u001b[39m\u001b[39mallow\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/storage/blob/_shared/policies.py:546\u001b[0m, in \u001b[0;36mStorageRetryPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msleep(retry_settings, request\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39mtransport)\n\u001b[1;32m    545\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m    547\u001b[0m \u001b[39mif\u001b[39;00m retry_settings[\u001b[39m'\u001b[39m\u001b[39mhistory\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m    548\u001b[0m     response\u001b[39m.\u001b[39mcontext[\u001b[39m'\u001b[39m\u001b[39mhistory\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m retry_settings[\u001b[39m'\u001b[39m\u001b[39mhistory\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/storage/blob/_shared/policies.py:520\u001b[0m, in \u001b[0;36mStorageRetryPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[39mwhile\u001b[39;00m retries_remaining:\n\u001b[1;32m    519\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m    521\u001b[0m         \u001b[39mif\u001b[39;00m is_retry(response, retry_settings[\u001b[39m'\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m    522\u001b[0m             retries_remaining \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mincrement(\n\u001b[1;32m    523\u001b[0m                 retry_settings,\n\u001b[1;32m    524\u001b[0m                 request\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mhttp_request,\n\u001b[1;32m    525\u001b[0m                 response\u001b[39m=\u001b[39mresponse\u001b[39m.\u001b[39mhttp_response)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "    \u001b[0;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 70 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/storage/blob/_shared/policies.py:313\u001b[0m, in \u001b[0;36mStorageResponseHook.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    308\u001b[0m     upload_stream_current \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mupload_stream_current\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    310\u001b[0m response_callback \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mresponse_callback\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m    311\u001b[0m     request\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mraw_response_hook\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_callback)\n\u001b[0;32m--> 313\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m    315\u001b[0m will_retry \u001b[39m=\u001b[39m is_retry(response, request\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m    316\u001b[0m \u001b[39m# Auth error could come from Bearer challenge, in which case this request will be made again\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:101\u001b[0m, in \u001b[0;36m_TransportRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend\u001b[39m(\u001b[39mself\u001b[39m, request: PipelineRequest[HTTPRequestType]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PipelineResponse[HTTPRequestType, HTTPResponseType]:\n\u001b[1;32m     92\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"HTTP transport send method.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \n\u001b[1;32m     94\u001b[0m \u001b[39m    :param request: The PipelineRequest object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m    :rtype: ~azure.core.pipeline.PipelineResponse\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[39mreturn\u001b[39;00m PipelineResponse(\n\u001b[1;32m    100\u001b[0m         request\u001b[39m.\u001b[39mhttp_request,\n\u001b[0;32m--> 101\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sender\u001b[39m.\u001b[39;49msend(request\u001b[39m.\u001b[39;49mhttp_request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49moptions),\n\u001b[1;32m    102\u001b[0m         context\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mcontext,\n\u001b[1;32m    103\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/transport/_requests_basic.py:364\u001b[0m, in \u001b[0;36mRequestsTransport.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m     error \u001b[39m=\u001b[39m ServiceRequestError(err, error\u001b[39m=\u001b[39merr)\n\u001b[1;32m    363\u001b[0m \u001b[39mif\u001b[39;00m error:\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mraise\u001b[39;00m error\n\u001b[1;32m    365\u001b[0m \u001b[39mif\u001b[39;00m _is_rest(request):\n\u001b[1;32m    366\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mazure\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrest\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_requests_basic\u001b[39;00m \u001b[39mimport\u001b[39;00m RestRequestsTransportResponse\n",
      "\u001b[0;31mServiceRequestError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x140512820>: Failed to resolve 'model-training' ([Errno 8] nodename nor servname provided, or not known)"
     ]
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential \n",
    "from azure.storage.blob import BlobClient\n",
    "\n",
    "# Storage Account name\n",
    "account_name = os.environ['DATALAKE_ACCOUNT_NAME']\n",
    "# Storage Account key\n",
    "account_key = os.environ['DATALAKE_ACCOUNT_KEY']\n",
    "\n",
    "# Name of the container where the blob is stored\n",
    "container_name = \"model-training\"\n",
    "\n",
    "# Name of the blob you want to fetch\n",
    "blob_name = \"DataShare/data1/2023-03-01-to-2023-03-31/events/events.csv\"\n",
    "\n",
    "file_path = \"../data/original/\"\n",
    "file_name = \"events.csv\"\n",
    "\n",
    "\n",
    "account_url = \"https://{account_key}.blob.core.windows.net\"\n",
    "# Create a BlobClient object with data transfer options for download\n",
    "blob_client = BlobClient(\n",
    "    account_url=account_url, \n",
    "    container_name=container_name, \n",
    "    blob_name=blob_name,\n",
    "    credential=account_key, #DefaultAzureCredential(),\n",
    "    max_single_get_size=1024*1024*32, # 32 MiB\n",
    "    max_chunk_get_size=1024*1024*4 # 4 MiB\n",
    ")\n",
    "\n",
    "with open(file=os.path.join(r'../askskan/data/original/', 'events.csv'), mode=\"wb\") as sample_blob:\n",
    "    download_stream = blob_client.download_blob(max_concurrency=2)\n",
    "    sample_blob.write(download_stream.readall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
