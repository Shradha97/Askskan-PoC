{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tiktoken\n",
    "# ! pip install python-dotenv\n",
    "# ! pip install pandas\n",
    "# ! pip install pandasql\n",
    "# ! pip install openai\n",
    "# ! pip install langchain\n",
    "# ! pip install langchain\\[all\\]\n",
    "# ! pip install pyspark\n",
    "# ! pip install findspark\n",
    "# ! pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some questions to ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_1 = f\"\"\"Who has the longest average processing time?\"\"\"\n",
    "# question_2 = f\"\"\"Who has the shortest average processing time?\"\"\"\n",
    "# question_3 = f\"\"\"Who is the most productive participant based on the number of tasks completed?\"\"\"\n",
    "# question_4 = f\"\"\"Who uses the least number of keystrokes on average?\"\"\"\n",
    "# question_5 = f\"\"\"Name of the participant that uses the most number of long cut keys on average?\"\"\" # An invalid question, to check hallucination\n",
    "# exception_question = f\"\"\"How was the day?\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LangChain for returning answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.document_loaders.text import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4 works with Azure ChatOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Azure keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "api_type = 'azure'\n",
    "model_deployment_name=os.getenv(\"AZURE_OPENAI_DNAME\")\n",
    "embeddings_deployment_name= os.getenv(\"AZURE_OPENAI_EMBED_NAME\")\n",
    "embedding_api_version='2022-12-01'\n",
    "model_api_version='2023-05-15'\n",
    "\n",
    "max_tokens = 1000\n",
    "chat_temperature = 0\n",
    " #This will correspond to the custom name you chose for your deployment when you deployed a model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read schema in JSON format\n",
    "Why only JSON?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_schema_file = '/Users/saishradhamohanty/Desktop/Repo/Prototypes/askskan/data/original/input_tables/pruned_events_schema.csv'\n",
    "abstraction_instances_schema_file = '/Users/saishradhamohanty/Desktop/Repo/Prototypes/askskan/data/original/input_tables/abstraction_instances/schema.txt'\n",
    "worktime_metrics_schema_file = '/Users/saishradhamohanty/Desktop/Repo/Prototypes/askskan/data/original/input_tables/worktime_metrics/schema.txt'\n",
    "\n",
    "major_tables_metadata_file = '/Users/saishradhamohanty/Desktop/Repo/Prototypes/askskan/data/original/metadata/major_tables_metadata.yaml'\n",
    "\n",
    "events_definitions_text_file = '/Users/saishradhamohanty/Desktop/Repo/Prototypes/askskan/data/original/input_tables/pruned_events_definitions.txt'\n",
    "abstraction_instances_definitions_text_file = '/Users/saishradhamohanty/Desktop/Repo/Prototypes/askskan/data/original/input_tables/abstraction_instances/definitions.txt'\n",
    "worktime_metrics_definitions_text_file = '/Users/saishradhamohanty/Desktop/Repo/Prototypes/askskan/data/original/input_tables/worktime_metrics/definitions.txt'\n",
    "\n",
    "fake_data_file = '/Users/saishradhamohanty/Desktop/Repo/Prototypes/askskan/data/original/sample_data.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate fake data based on original schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import pandas as pd\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Generate 10 unique rows of data\n",
    "data = []\n",
    "unique_values = set()\n",
    "\n",
    "while len(data) < 20:\n",
    "    row = {\n",
    "        \"event_id\": fake.uuid4(),\n",
    "        \"sequence_id\": fake.random_int(min=10000, max=99999),\n",
    "        \"event_time\": fake.iso8601(),\n",
    "        \"persona_name\": fake.name(),\n",
    "        \"app_name\": fake.word(),\n",
    "        \"agent_type\": fake.random_int(min=0, max=200),\n",
    "        \"clipboard\": fake.random_int(min=0, max=10),\n",
    "        \"participant_name\": fake.name(),\n",
    "        \"title\": fake.sentence(),\n",
    "        \"event_date\": fake.date(),\n",
    "        \"navigation_key_count\": fake.random_int(min=0, max=10),\n",
    "        \"number_key_count\": fake.random_int(min=0, max=10),\n",
    "        \"mouse_count\": fake.random_int(min=0, max=10),\n",
    "        \"mouse_wheel\": fake.random_int(min=0, max=10),\n",
    "        \"alpha_key_count\": fake.random_int(min=0, max=10),\n",
    "        \"active_time\": fake.random_int(min=0, max=100),\n",
    "        \"idle_time\": random.uniform(0, 2000)/100.0, # fake.random_number(max=2000) / 100.0,\n",
    "        \"wait_time\": random.uniform(0, 2000)/100.0,\n",
    "        \"processing_time\": random.uniform(0, 1), # fake.random_number(max=1),\n",
    "        \"TaT_event\": random.uniform(0, 1),\n",
    "        \"session_switch\": fake.random_int(min=0, max=1),\n",
    "        \"app_switch\": fake.random_int(min=0, max=1),\n",
    "        \"case_switch\": fake.random_int(min=0, max=1),\n",
    "        \"activity_id\": fake.uuid4(),\n",
    "        \"activity_abstraction_level_id\": fake.uuid4(),\n",
    "        \"activity_abstraction_level_name\": fake.word(),\n",
    "        \"parent_activity_id\": fake.uuid4(),\n",
    "        \"activity_discovered_name\": fake.sentence(),\n",
    "        \"activity_alias_name\": fake.sentence(),\n",
    "        \"activity_instance_id\": fake.uuid4(),\n",
    "        \"activity_instance_abstraction_level_alias_name\": fake.word(),\n",
    "        \"activity_instance_original_end_time\": fake.iso8601(),\n",
    "        \"activity_instance_end_time\": fake.iso8601(),\n",
    "        \"activity_instance_event_count\": fake.random_int(min=1, max=10),\n",
    "        \"activity_instance_start_time\": fake.iso8601(),\n",
    "        \"case_id_name\": fake.word(),\n",
    "        \"case_id_value\": fake.word(),\n",
    "        \"url\": fake.url(),\n",
    "        \"is_pruned\": fake.random_int(min=0, max=1),\n",
    "        \"source\": fake.random_element([\"UA\", \"AA\", \"EA\"]),\n",
    "        \"event_control_type\": fake.random_int(min=10000, max=99999),\n",
    "        \"event_input_type\": fake.random_int(min=100, max=999),\n",
    "    }\n",
    "    \n",
    "    # Check if the generated row is unique\n",
    "    row_tuple = tuple(row.values())\n",
    "    if row_tuple not in unique_values:\n",
    "        data.append(row)\n",
    "        unique_values.add(row_tuple)\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataset\n",
    "# print(df.head())\n",
    "\n",
    "# Convert DataFrame to CSV\n",
    "df.to_csv('../data/original/sample_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert csv to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = definition_file\n",
    "text_file = '../data/original/definitions.txt'\n",
    "\n",
    "with open(csv_file, 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    terms_definitions = []\n",
    "    for row in reader:\n",
    "        term = row['Term'].strip()\n",
    "        definition = row['Definition'].strip()\n",
    "        terms_definitions.append(f'{term}:{definition}')\n",
    "\n",
    "with open(text_file, 'w') as file:\n",
    "    file.write('\\n'.join(terms_definitions))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Specify the input text file path\n",
    "input_file = file0\n",
    "\n",
    "# Specify the output CSV file path\n",
    "output_file = '../data/sample/schema0.csv'\n",
    "\n",
    "# Define the field names for the CSV file\n",
    "field_names = [\n",
    "    'case_id',\n",
    "    'case_status',\n",
    "    'case_type',\n",
    "    'task_name',\n",
    "    'participant_id',\n",
    "    'process_name',\n",
    "    'process_variant',\n",
    "    'applications_used',\n",
    "    'processing_time',\n",
    "    'wait_time',\n",
    "    'turnaround_time',\n",
    "    'start_time',\n",
    "    'no_of_keystrokes',\n",
    "    'no_of_shortcut_keys'\n",
    "]\n",
    "\n",
    "# Open the input file for reading\n",
    "with open(input_file, 'r') as file:\n",
    "    # Read the lines from the input file\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Create a list to store the data rows for the CSV\n",
    "data_rows = []\n",
    "\n",
    "# Parse each line and extract the values\n",
    "for line in lines:\n",
    "    # Remove leading/trailing whitespaces and split the line by ':'\n",
    "    line = line.strip()\n",
    "    parts = line.split(':')\n",
    "\n",
    "    # Extract the field name and value\n",
    "    field_name = parts[0].strip()\n",
    "    field_value = ':'.join(parts[1:]).strip()\n",
    "\n",
    "    # Append the extracted values to the data row\n",
    "    data_row = field_value\n",
    "    data_rows.append(data_row)\n",
    "\n",
    "# Write the data rows to the output CSV file\n",
    "with open(output_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header row\n",
    "    writer.writerow(field_names)\n",
    "    \n",
    "    # Write the data rows\n",
    "    writer.writerow(data_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file and skip the first row\n",
    "df = pd.read_excel(file2, skiprows=1)\n",
    "\n",
    "# Drop the desired column\n",
    "column_to_drop = ['Source', 'Comments']\n",
    "df = df.drop(column_to_drop, axis=1)\n",
    "\n",
    "# Convert DataFrame to CSV\n",
    "df.to_csv('../data/original/schema1.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load documents that require embedding creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_csv = CSVLoader(file_path=events_schema_file)\n",
    "docs_csv = loader_csv.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read other schema files in the text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the text file\n",
    "file_path = abstraction_instances_schema_file\n",
    "\n",
    "# Read the contents of the text file\n",
    "with open(file_path, 'r') as file:\n",
    "    abstraction_instances_schema = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the text file\n",
    "file_path = worktime_metrics_schema_file\n",
    "\n",
    "# Read the contents of the text file\n",
    "with open(file_path, 'r') as file:\n",
    "    worktime_metrics_schema = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'major tables': [{'name': 'unum_askskan.events_delta_tb', 'description': 'This table contains information related to events', 'usage': 'Use this table when asked about events for any persona', 'columns': ['event_id', 'event_time', 'sequence_id', 'persona_id', 'participant_id', 'application_id', 'agent_type', 'clipboard', 'title', 'event_date', 'navigation_key_count', 'number_key_count', 'mouse_count', 'mouse_wheel', 'alpha_key_count'], 'additional_minor_tables': ['describe_table_abstraction_instances', 'describe_worktime_metrics'], 'table_join_info': [{'join_set_1': [{'source_column': 'persona_id'}, {'target_column': 'persona_id'}, {'target_table': 'describe_table_abstraction_instances'}]}, {'join_set_2': [{'source_column': 'participant_id'}, {'target_column': 'participant_id'}, {'target_table': 'describe_worktime_metrics'}]}]}]}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Read YAML data from a file\n",
    "input_yaml_file = major_tables_metadata_file\n",
    "with open(input_yaml_file, 'r') as yaml_file:\n",
    "    yaml_data = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "print(yaml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "major tables:\n",
      "  - name: unum_askskan.events_delta_tb\n",
      "    description: \"This table contains information related to events\"\n",
      "    usage: \"Use this table when asked about events for any persona\"\n",
      "    columns:\n",
      "      - event_id\n",
      "      - event_time\n",
      "      - sequence_id\n",
      "      - persona_id\n",
      "      - participant_id\n",
      "      - application_id\n",
      "      - agent_type\n",
      "      - clipboard\n",
      "      - title\n",
      "      - event_date\n",
      "      - navigation_key_count\n",
      "      - number_key_count\n",
      "      - mouse_count\n",
      "      - mouse_wheel\n",
      "      - alpha_key_count\n",
      "    additional_minor_tables:\n",
      "      - describe_table_abstraction_instances\n",
      "      - describe_worktime_metrics\n",
      "    table_join_info:\n",
      "      - join_set_1:\n",
      "          - source_column: \"persona_id\"\n",
      "          - target_column: \"persona_id\"\n",
      "          - target_table: \"describe_table_abstraction_instances\"\n",
      "      - join_set_2:\n",
      "          - source_column: \"participant_id\"\n",
      "          - target_column: \"participant_id\"\n",
      "          - target_table: \"describe_worktime_metrics\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read YAML data as text from a file\n",
    "input_yaml_file = major_tables_metadata_file\n",
    "with open(input_yaml_file, 'r') as yaml_file:\n",
    "    table_metadata_text = yaml_file.read()\n",
    "\n",
    "print(table_metadata_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the text files of definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the text file\n",
    "file_path = events_definitions_text_file\n",
    "\n",
    "# Read the contents of the text file\n",
    "with open(file_path, 'r') as file:\n",
    "    events_definitions = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the text file\n",
    "file_path = abstraction_instances_definitions_text_file\n",
    "\n",
    "# Read the contents of the text file\n",
    "with open(file_path, 'r') as file:\n",
    "    abstraction_instances_definitions = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the text file\n",
    "file_path = worktime_metrics_definitions_text_file\n",
    "\n",
    "# Read the contents of the text file\n",
    "with open(file_path, 'r') as file:\n",
    "    worktime_metrics_definitions = file.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7, chunk_overlap=6)\n",
    "documents = text_splitter.split_documents(docs_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<class 'openai.api_resources.embedding.Embedding'> model='text-embedding-ada-002' deployment='architecture4I93pDgC2s-textemb' openai_api_version='2022-12-01' openai_api_base='https://skandevazopenai.openai.azure.com/' openai_api_type='azure' openai_proxy='' embedding_ctx_length=8191 openai_api_key='09ec6bb5136a4afab27de6a44fa4f998' openai_organization='' allowed_special=set() disallowed_special='all' chunk_size=16 max_retries=6 request_timeout=None headers=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Requests to the Embeddings_Create Operation under Azure OpenAI API version 2022-12-01 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 6 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Requests to the Embeddings_Create Operation under Azure OpenAI API version 2022-12-01 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 2 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "                deployment=embeddings_deployment_name,\n",
    "                openai_api_key=api_key,\n",
    "                openai_api_base=api_base,\n",
    "                openai_api_type=api_type,\n",
    "                openai_api_version=embedding_api_version,\n",
    "                chunk_size=16,\n",
    "            )\n",
    "print(embeddings)\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create template to provide as an input to the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_table_prompt_v1 = \"\"\"\n",
    "You are an intelligent data analyst.\n",
    "\n",
    "1. You are given table metadata delimited by <> in a YAML format that contains data about all the \\\n",
    "    available tables in the database.  \n",
    "2. Every table entry under the tables section includes:\n",
    "    - name: Identifies the table uniquely.\n",
    "    - description: Provides a concise summary of the table's purpose and content.\n",
    "    - usage: Offers insights into when one should utilize the information in the table.\n",
    "    - columns: List of all the columns in the table.\n",
    "3. For every table entry, information about how to join different possible tables join is specified in the table_join_info \\\n",
    "    section, information about different sets of joins is given in the join_set_i section, where i is the join set number.\\\n",
    "    For every join set, the following information is provided:\n",
    "    - table_join_info: This field gives information about the columns that would be used for joining 2 different tables.\\\n",
    "        This is a list of tuples where:\n",
    "            source_column: the first element of the tuple is the column in the current table that would be needed for joining with the \\\n",
    "                referenced table.\n",
    "            target_column: the second element of the tuple is the column in the referenced table that would be needed for joining with the \\\n",
    "                current table.\n",
    "            target_table: the third element is the name of the referenced table.\n",
    "5. Strictly follow the following instructions:\n",
    "    - You are given a question by the human delimited by <>.\n",
    "    - Use the table metadata to identify the relevant tables in the database that would be needed to be \\\n",
    "        joined to answer the question based on the following conditions.\n",
    "        - If the question can be answered using a single table then select only that table.\n",
    "        - If the question needs information from multiple tables for answering the user question, then select the \\\n",
    "            minimum set of the relevant tables that can answer the question fully.\n",
    "    - Return the selected table(s) in a list, refer to the example output below. Never include any extra text.\n",
    "    - The table name should only be selected from the name field in the table metadata.\n",
    "    - Omit any explanations.\n",
    "\n",
    "Table metadata: <{table_metadata}>\n",
    "\n",
    "Once the relevant table(s) are selected, use the following list format strictly to return the result, refer to the \\ \n",
    "example output:\n",
    "Output: [\"events\", \"describe_table_abstraction_instances\"]\n",
    "\n",
    "Current conversation: \n",
    "Human: <{question}>\n",
    "Table(s) selected: \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_table_prompt_v2 = \"\"\"\n",
    "You are an intelligent data analyst.\n",
    "\n",
    "Table metadata: <{table_metadata}>\n",
    "\n",
    "1. You are given table metadata delimited by <> in a YAML format that contains data about all the \\\n",
    "    available tables in the database.  \n",
    "2. Every table entry under the major tables section includes:\n",
    "    - name: Identifies the major table uniquely.\n",
    "    - description: Provides a concise summary of the table's purpose and content.\n",
    "    - usage: Offers insights into when one should utilize the information in the table.\n",
    "    - columns: List of all the columns in the table.\n",
    "    - additional_minor_tables: List of all the additional minor tables that should be joined with the current major table.\n",
    "3. For every table entry, information about how to join different possible tables is specified in the table_join_info \\\n",
    "    section, information about different sets of joins is given in the join_set_i section, where i is the join set number.\\\n",
    "    For every join set, the following information is provided:\n",
    "    - table_join_info: This field gives information about the columns that would be used for joining 2 different tables. The format is:\n",
    "            source_column: the first element of the tuple is the column in the current major table that would be needed for joining with the \\\n",
    "                referenced table.\n",
    "            target_column: the second element of the tuple is the column in the referenced minor table that would be needed \\\n",
    "                for joining with the current table.\n",
    "            target_table: the third element is the name of the referenced minor table.\n",
    "4. For obtaining the relevant tables strictly follow the following instruction step by step:\n",
    "    - You are given a question by the human delimited by <>.\n",
    "    - Return the selected table(s) as a list of list in the Output section using the steps listed below strictly:\n",
    "        - First select the major table(s) that would be needed to answer the question.\n",
    "        - Select the list of additional minor tables corresponding to the major table(s) selected.\n",
    "        - You should create an inner list for each additional minor table of the selected major table irrespective of \\\n",
    "            whether the additional minor table is needed for answering the question or not.\n",
    "        - For every minor table in the additional minor table list, you should create an inner list such that:\n",
    "            - The major table for this minor table is the first element.\n",
    "            - This minor table is the second element.\n",
    "            - The column in the major table and the minor table that would be used for joining with the additional minor table. \\\n",
    "            Take this column from the target_column and source column fields under the table_join_info field in the table metadata.\n",
    "        - Repeat the above step for each selected major table.\n",
    "    _ Never assume anything on your own, just follow the above given instructions thoroughly.\n",
    "    - Never include any extra text.\n",
    "    - Omit any reasoning.\n",
    "\n",
    "Following are examples of the user question along with the user question(s) and corresponding desired output. \\\n",
    "    Return the desired output by prepending the Output: keyword to the output as shown below.:\n",
    "Question: \"Which persona has the greatest worktime that is not whitelisted?\"\n",
    "Output: [[\"events\", \"describe_table_abstraction_instances\", \"persona_id\"], [\"events\", \"describe_worktime_metrics\", \"participant_id\"]]\n",
    "Reasoning: As the question needs information about persona, it will be obtained from the events table as the major table. \\\n",
    "            Since the instruction says to select the all of the additional minor tables corresponding to the major table, \\\n",
    "            hence, create an inner list for each additional minor table of the selected major table. \\\n",
    "\n",
    "Current conversation: \n",
    "Human: <{question}>\n",
    "Table(s) selected: Output: \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_table_prompt_v3 = \"\"\"\n",
    "You are an intelligent data analyst.\n",
    "\n",
    "Table metadata: <{table_metadata}>\n",
    "\n",
    "1. You are given table metadata delimited by <> in a YAML format that contains data about all the \\\n",
    "    available tables in the database.  \n",
    "2. Every table entry under the major tables section includes:\n",
    "    - name: Identifies the major table uniquely.\n",
    "    - description: Provides a concise summary of the table's purpose and content.\n",
    "    - usage: Offers insights into when one should utilize the information in the table.\n",
    "    - columns: List of all the columns in the table.\n",
    "    - additional_minor_tables: List of all the additional minor tables that should be joined with the current major table.\n",
    "3. For every table entry, information about how to join different possible tables is specified in the table_join_info \\\n",
    "    section, information about different sets of joins is given in the join_set_i section, where i is the join set number.\\\n",
    "    For every join set, the following information is provided:\n",
    "    - table_join_info: This field gives information about the columns that would be used for joining 2 different tables. The format is:\n",
    "            source_column: the first element of the tuple is the column in the current major table that would be needed for joining with the \\\n",
    "                referenced table.\n",
    "            target_column: the second element of the tuple is the column in the referenced minor table that would be needed \\\n",
    "                for joining with the current table.\n",
    "            target_table: the third element is the name of the referenced minor table.\n",
    "4. For obtaining the relevant tables strictly follow the following instruction step by step:\n",
    "    - You are given a question by the human delimited by <>.\n",
    "    - Return the selected table(s) as a dictionary of a list of list in the Output section using the steps listed below strictly:\n",
    "        - First select the major table(s) that would be needed to answer the question.\n",
    "        - Select the list of additional minor tables corresponding to the major table(s) selected.\n",
    "        - Create a dictionary with the major table(s) as the key and a list of list as the value. This list of list would be such that:\n",
    "            - The number of inner lists would be equal to the number of additional minor tables corresponding to the major table.\n",
    "            - For every minor table in the additional minor table list for the major table, you should create an inner list such that:\n",
    "                - This minor table is the first element.\n",
    "                - The column in the major table and the minor table that would be used for joining with the additional minor table as the third element. \\\n",
    "                Take this column from the target_column and source column fields under the table_join_info field in the table metadata.\n",
    "        - Repeat the above step for each selected major table.\n",
    "    _ Never assume anything on your own, just follow the above given instructions thoroughly.\n",
    "    - Never include any extra text.\n",
    "    - Omit any reasoning.\n",
    "\n",
    "Following are examples of the user question along with the user question(s) and corresponding desired output. \\\n",
    "    Return the desired output by prepending the Output: keyword to the output as shown below.:\n",
    "Question: \"Which persona has the greatest worktime that is not whitelisted?\"\n",
    "Output: {{\n",
    "            \"unum_askskan.events_delta_tb\": [[\"describe_table_abstraction_instances\", \"persona_id\"], [\"describe_worktime_metrics\", \"participant_id\"]]\n",
    "        }}\n",
    "Reasoning: As the question needs information about persona, it will be obtained from the events table as the major table that forms a key of the \\\n",
    "            dictionary. Since the instruction says to select the all of the additional minor tables corresponding to the major table, \\\n",
    "            hence, create an inner list for each additional minor table of the selected major table. \\\n",
    "\n",
    "Current conversation: \n",
    "Human: <{question}>\n",
    "Table(s) selected: Output: \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_generation_prompt_with_tables = \"\"\"\n",
    "You are a super smart code generator.\n",
    "Perform the following actions:\n",
    "\n",
    "1. Carefully understand the question by human, schema, schema definitions and context each delimited by <>.\n",
    "2. Do not assume or create any schema attributes not given in the schema unless stated explicitly.\n",
    "3. Do not assume any facts from the question unless stated explicitly.\n",
    "4. Do not assume any values of the schema attributes that are not present in the schema or in the question. \n",
    "5. Strictly follow the following instructions:\n",
    "    - The data table dictionary delimited by <> is a dictionary of list of list where:\n",
    "        - The key is the name of the major table.\n",
    "        - The value is a list of list where:\n",
    "            - The first element is a minor table that needs to be joined with the major table.\n",
    "            - The second element is the column in the major table and minor table that would be used for joining with the minor table.\n",
    "    - The schema has names of the schema columns in Column Name, the descriptions of the columns in Description \\\n",
    "        and an example value of the schema column in Examples.\n",
    "    - Use only the Column Name in the schema and the question to generate the final Spark SQL query from the tables in  \\\n",
    "        the data table dictionary for answering the original question. To join the tables, follow the instructions given below:\n",
    "        - For every major table in the data table dictionary, join all the minor tables in the value list \\\n",
    "            of the major table to the major table.\n",
    "        - Join the minor table with the major table using the column given along with the minor table in the value list of the major table.\n",
    "    - Schema definitions and Description in schema can be referred for providing more clarity about Columns in the schema. \\\n",
    "        Strictly do not use values from schema definitions as column names or column values to generate the query.\n",
    "    - The query result should be LIMITED by 10 rows.\n",
    "    - The generated query should be from the start date to end date each delimited by <>. \n",
    "6. Output the correct Spark SQL query.\n",
    "7. Select all the appropriate column(s) from the generated Spark SQL query by critically referring to the \\\n",
    "    requirements asked in the original question.\n",
    "8. Write the answer in a friendly tone in response to the question.\n",
    "9. Stricly use the following JSON schema format for the output, refer to the example output below. Never include any\\\n",
    "extra text.\n",
    "10. Omit any explanations.\n",
    "\n",
    "Events schema: <{context}>\n",
    "Describe table abstraction instances schema: <{abstraction_instances_schema}>\n",
    "Describe worktime metrics schema: <{worktime_metrics_schema}>\n",
    "Events schema definitions: <{events_schema_definitions}>\n",
    "Describe table abstraction instances schema definitions: <{abstraction_instances_schema_definitions}>\n",
    "Describe worktime metrics schema definitions: <{worktime_metrics_schema_definitions}>\n",
    "Data table dictionary: <{data_table_dictionary}>\n",
    "Start date: <{start_date}>\n",
    "End date: <{end_date}>\n",
    "\n",
    "\n",
    "Once the Spark SQL query generated use only the following JSON schema format stricly, refer to the example output:\n",
    "Output: {{\n",
    "    \"Query\": The generated Spark SQL query from the start date to end date. Limit the results to maximum 10 rows for queries \\\n",
    "        with more than 1 row,\n",
    "    \"Column\": The correct extracted column(s) from the Spark SQL query in a list,\n",
    "    \"Skan Bot\": The final answer in a friendly tone. The answer from the SQL query should be delimited by ##.\n",
    "}}\n",
    "\n",
    "Following is an example of the output:\n",
    "Output: {{\n",
    "    \"Query\": \"SELECT app_name, COUNT(*) AS count FROM clipboard WHERE event_date >= '2023-04-01' AND event_date <= '2023-04-30'\" \\\n",
    "        GROUP BY app_name ORDER BY count DESC LIMIT 1',\n",
    "    \"Column\": \"app_name\", \n",
    "    \"Skan Bot\": \"The most used application is #result#.\"\n",
    "}}\n",
    "\n",
    "Output: {{\n",
    "    \"Query\": \"SELECT persona_name, SUM(worktime_whitelisted) AS total_whitelisted_worktime \\\n",
    "                FROM unum_askskan.events_delta_tb AS events_delta_tb \\\n",
    "                JOIN describe_worktime_metrics ON events_delta_tb.participant_id = describe_worktime_metrics.participant_id \\\n",
    "                WHERE event_date >= '2023-04-01' AND event_date <= '2023-04-30' \\\n",
    "                GROUP BY persona_name \\\n",
    "                ORDER BY total_whitelisted_worktime DESC \\\n",
    "                LIMIT 1\",\n",
    "    \"Column\": \"persona_name\", \n",
    "    \"Skan Bot\": \"The persona with the highest whitelisted worktime is #result#.\"\n",
    "}}\n",
    "\n",
    "Please use \"double quotes\" for json keys and ensure the Output can be parsed by Python json.loads\n",
    "    \n",
    "\n",
    "Current conversation:\n",
    "{chat_history}   \n",
    "Human: <{question}>\n",
    "Skan Bot: \n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions to be asked by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = \"Which persona has the greatest worktime whitelisted?\"\n",
    "question_2 = \"Total time spent of non process application?\"\n",
    "question_3 = \"Is there an outstanding performer in CES persona?\"\n",
    "question_4 = \"Which is the most used application?\"\n",
    "question_5 = \"Which is the most active time window during the day?\"\n",
    "question_6 = \"What is the average case effort per persona?\"\n",
    "question_7 = \"What is the average utilization per persona?\"\n",
    "question_8 = \"Which week had highest productivity in CES?\"\n",
    "question_9 = \"What is the distribution of time spent in processing application per day?\"\n",
    "question_10 = \"What is the average number of participants per case?\"\n",
    "question_11 = \"What is the frequency of cases per participant per day?\"\n",
    "question_12 = \"Who will be most productive next week?\"\n",
    "question_13 = \"Any patterns observed that give an insight on inefficiencies?\"\n",
    "question_14 = \"Would taking breaks improve the efficiency in performance?\"\n",
    "\n",
    "question_15 = \"What is the standard deviation of time spent on process application?\"\n",
    "question_16 = \"What is the standard deviation of time spent on non process application?\"\n",
    "\n",
    "# question_1 = f\"\"\"Who has the longest average processing time?\"\"\"\n",
    "# question_2 = f\"\"\"Who has the shortest average processing time?\"\"\"\n",
    "# question_3 = f\"\"\"Who is the most productive participant based on the number of tasks completed?\"\"\"\n",
    "# question_4 = f\"\"\"Who uses the least number of keystrokes on average?\"\"\"\n",
    "# question_5 = f\"\"\"Name of the participant that uses the most number of long cut keys on average?\"\"\" # An invalid question, to check hallucination\n",
    "exception_question = f\"\"\"How was the day?\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Providing output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "class FormatCodeOutput(BaseModel):\n",
    "    Query: str = Field(description=\"The generated Spark SQL query\")\n",
    "    Code: str = Field(description=\"Python code generated to run the Spark SQL query\")\n",
    "    Skan_Bot: str = Field(description=\"The final answer printed by the python code in a friendly tone. Place the answer between ##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormatDoubtOutput(BaseModel):\n",
    "    Doubt: str = Field(description=\"The clarification point asked by the bot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Azure LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LangChain with Azure OpenAI\n",
    "chat_llm = AzureChatOpenAI(\n",
    "    deployment_name=model_deployment_name,\n",
    "    openai_api_version=model_api_version,\n",
    "    openai_api_base=api_base,\n",
    "    openai_api_key=api_key,\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "table_selection_prompt = PromptTemplate(\n",
    "    template=zero_shot_table_prompt_v3,\n",
    "    input_variables=[\n",
    "        \"question\",\n",
    "    ],\n",
    "    partial_variables={\"table_metadata\": table_metadata_text},\n",
    "    output_parser=CommaSeparatedListOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the first chain for obtaining relevant tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\n",
      "            \"unum_askskan.events_delta_tb\": [[\"describe_table_abstraction_instances\", \"persona_id\"], [\"describe_worktime_metrics\", \"participant_id\"]]\n",
      "        }\n"
     ]
    }
   ],
   "source": [
    "table_selection_chain = LLMChain(prompt=table_selection_prompt, llm=chat_llm, verbose=False)\n",
    "data_table_dict = table_selection_chain.predict(question=\"Which persona has the greatest whitelisted worktime?\")\n",
    "print(data_table_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "            \"unum_askskan.events_delta_tb\": [[\"describe_table_abstraction_instances\", \"persona_id\"], [\"describe_worktime_metrics\", \"participant_id\"]]\n",
      "        }\n"
     ]
    }
   ],
   "source": [
    "string = data_table_dict\n",
    "\n",
    "# Find and remove the first occurrence of \"Query\" in the string\n",
    "stripped_output_section = string.replace(\n",
    "                    'Output:', \"\").strip()\n",
    "print(stripped_output_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unum_askskan.events_delta_tb': [['describe_table_abstraction_instances', 'persona_id'], ['describe_worktime_metrics', 'participant_id']]}\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "# Using ast.literal_eval to safely evaluate the string as a literal Python expression\n",
    "result_tables_dict = ast.literal_eval(stripped_output_section)\n",
    "print(result_tables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the 2nd chain for using tables and question etc to generate the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_generation_prompt = PromptTemplate(\n",
    "    template=query_generation_prompt_with_tables,\n",
    "    input_variables=[\n",
    "        \"context\",\n",
    "        \"question\",\n",
    "        \"chat_history\",\n",
    "        \"data_table_dictionary\"\n",
    "    ],\n",
    "    partial_variables={\"events_schema_definitions\": events_definitions,\n",
    "                        \"abstraction_instances_schema_definitions\": abstraction_instances_definitions,\n",
    "                        \"worktime_metrics_schema_definitions\": worktime_metrics_definitions_text_file,\n",
    "                        \"abstraction_instances_schema\": abstraction_instances_schema,\n",
    "                        \"worktime_metrics_schema\": worktime_metrics_schema,\n",
    "                        \"start_date\": \"2023-04-01\",\n",
    "                        \"end_date\": \"2023-04-30\"}\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": query_generation_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\")\n",
    "query_generation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm = chat_llm, \n",
    "    retriever = retriever, \n",
    "    memory=buffer_memory,\n",
    "    combine_docs_chain_kwargs=chain_type_kwargs,\n",
    "    verbose=False,\n",
    "    get_chat_history=lambda h:h\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_generation_chain_input = {\n",
    "    \"question\": \"Which persona has the highest turnaround time?\",\n",
    "    \"data_table_dictionary\": result_tables_dict,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Query\": \"SELECT persona_name, AVG(turnaround_time) AS avg_turnaround_time FROM unum_askskan.events_delta_tb JOIN describe_table_abstraction_instances ON unum_askskan.events_delta_tb.persona_id = describe_table_abstraction_instances.persona_id WHERE abstraction_type = 'Activity' AND start_time >= '2023-04-01' AND end_time <= '2023-04-30' GROUP BY persona_name ORDER BY avg_turnaround_time DESC LIMIT 1\",\n",
      "    \"Column\": [\"persona_name\", \"avg_turnaround_time\"],\n",
      "    \"Skan Bot\": \"The persona with the highest turnaround time is #result#.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = query_generation_chain(query_generation_chain_input)\n",
    "print(result['answer'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import RetryWithErrorOutputParser\n",
    "retry_parser = RetryWithErrorOutputParser.from_llm(\n",
    "    parser=parser, llm=OpenAI(temperature=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PromptTemplate' object has no attribute 'to_string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/langchain/output_parsers/pydantic.py:25\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     24\u001b[0m     json_str \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup()\n\u001b[0;32m---> 25\u001b[0m json_object \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(json_str, strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     26\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpydantic_object\u001b[39m.\u001b[39mparse_obj(json_object)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m     kw[\u001b[39m'\u001b[39m\u001b[39mparse_constant\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m parse_constant\n\u001b[0;32m--> 359\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\u001b[39m.\u001b[39;49mdecode(s)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/langchain/output_parsers/retry.py:109\u001b[0m, in \u001b[0;36mRetryWithErrorOutputParser.parse_with_prompt\u001b[0;34m(self, completion, prompt_value)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     parsed_completion \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparser\u001b[39m.\u001b[39;49mparse(completion)\n\u001b[1;32m    110\u001b[0m \u001b[39mexcept\u001b[39;00m OutputParserException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/langchain/output_parsers/pydantic.py:31\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     30\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to parse \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m from completion \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m. Got: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[39mraise\u001b[39;00m OutputParserException(msg)\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Failed to parse FormatOutput from completion import pandas as pd\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\ndata = pd.read_csv('../askskan/data/original/sample_data.csv')\ndata_table = spark.createDataFrame(data)\ndata_table.createOrReplaceTempView('data_table')\n\nquery = \"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\"\nresult = spark.sql(query)\nresult.show()\n. Got: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[217], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m retry_parser\u001b[39m.\u001b[39;49mparse_with_prompt(code, new_prompt)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/langchain/output_parsers/retry.py:112\u001b[0m, in \u001b[0;36mRetryWithErrorOutputParser.parse_with_prompt\u001b[0;34m(self, completion, prompt_value)\u001b[0m\n\u001b[1;32m    109\u001b[0m     parsed_completion \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparser\u001b[39m.\u001b[39mparse(completion)\n\u001b[1;32m    110\u001b[0m \u001b[39mexcept\u001b[39;00m OutputParserException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    111\u001b[0m     new_completion \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry_chain\u001b[39m.\u001b[39mrun(\n\u001b[0;32m--> 112\u001b[0m         prompt\u001b[39m=\u001b[39mprompt_value\u001b[39m.\u001b[39;49mto_string(), completion\u001b[39m=\u001b[39mcompletion, error\u001b[39m=\u001b[39m\u001b[39mrepr\u001b[39m(e)\n\u001b[1;32m    113\u001b[0m     )\n\u001b[1;32m    114\u001b[0m     parsed_completion \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparser\u001b[39m.\u001b[39mparse(new_completion)\n\u001b[1;32m    116\u001b[0m \u001b[39mreturn\u001b[39;00m parsed_completion\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PromptTemplate' object has no attribute 'to_string'"
     ]
    }
   ],
   "source": [
    "retry_parser.parse_with_prompt(code, new_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "Human: Which is the most used application?\n",
      "AI: Doubt: Can you please clarify what you mean by \"most used application\"? Are you asking for the application with the highest active time or the application that was switched to the most frequently?\n",
      "Human: Appliction with highest active time.\n",
      "AI: Doubt: Are you asking for the application with the highest active time in general or for a specific time period?\n",
      "Human: In general\n",
      "AI: Output: The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "```\n",
      "{\n",
      "  \"Query\": \"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\",\n",
      "  \"Code\": \"import pandas as pd\\nfrom pyspark.sql import SparkSession\\n\\nspark = SparkSession.builder.getOrCreate()\\ndata = pd.read_csv('../askskan/data/original/sample_data.csv')\\ndata_table = spark.createDataFrame(data)\\ndata_table.createOrReplaceTempView('data_table')\\n\\nquery = \\\"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\\\"\\nresult = spark.sql(query)\\nresult.show()\\n\",\n",
      "  \"Skan_Bot\": \"The application with the highest active time is <application name> with an active time of <highest active time>.\"\n",
      "}\n",
      "```\n",
      "Human: Which is the most active time window during the day?\n",
      "AI: Doubt: Can you please clarify what you mean by \"active time window\"? Are you asking for the time period during the day with the highest active time?\n",
      "Human: Yes\n",
      "AI: Yes, I am asking for the time period during the day with the highest active time.\n",
      "Human: Yes, the time period during the day with the highest active time.\n",
      "AI: Doubt: Can you please specify the format in which you want the time period during the day to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in between 2 different times. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output format to be in between 2 different times. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in another format. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Follow Up Input: I want the output to be in another format.\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n",
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a super smart code generator.\n",
      "Perform the following actions:\n",
      "\n",
      "1. Understand the input by human, schema and schema definitions each delimited by <>\n",
      "2. Based on the question, the schema and the schema definitions, first seek to clarify any ambiguities.\n",
      "    - Assume human does not know anything about the schema, so never ask details about the schema or schema         definitions or technical details in the clarification points.\n",
      "    - First try to find the answer to the ambiguities from the schema and schema definitions yourself.\n",
      "    - If the ambiguities still persist then: \n",
      "        1. Without mentioning about the schema or schema definitions only think of a list of super short bullets of             areas that need clarification in simple terms. \n",
      "        2. Then pick one clarification point, and wait for an answer from the human before moving to the next point.\n",
      "3. Never ask the human to explain the schema or schema definitions.\n",
      "4. Only human can answer these clarification points.\n",
      "5. Do not assume any schema attributes unless stated explicitly.\n",
      "6. Only after all the clarification points are answered:\n",
      "    - Use only columns in schema to generate a final Spark SQL query answering the original question. Refer to the         schema definitions only for more clarity about terms in the schema.\n",
      "    - Output computationally most efficient Spark SQL query.\n",
      "7. You can ask maximum 7 clarification points.\n",
      "9. I have a pandas csv table in data table delimited by <> that contains the data to be queried.\n",
      "    - Output a valid python code to execute the generated Spark SQL query on the data file ending with a print statement         showing the final answer in a friendly tone.\n",
      "10. Omit any explanations.\n",
      "\n",
      "Schema: <Name: active_time\n",
      "Description: Active time measures the duration when a user is actively interacting with the computer by using the keyboard or mouse between two actions on the computer. It is calculated by dividing the time between the previous and current actions into one-minute intervals. If there is any keyboard or mouse activity within these intervals, that time is counted as part of the active time.\n",
      "Sample_Value: 0\n",
      "\n",
      "Name: processing_time\n",
      "Description: Total idle time below thresh hold-window + active_time (default thresh hold is 3 minutes). Processing time can also be referred to as active_time_in_view (This is different from active_time).\n",
      "Sample_Value: 0.72\n",
      "\n",
      "Name: idle_time\n",
      "Description: Total Time where there is no keyboard or mouse activity within a minute window\n",
      "Sample_Value: 1838.22\n",
      "\n",
      "Name: activity_instance_original_end_time\n",
      "Description: End time is the time of the last event in the Activity Instance. The event can be a user event or non actionable event. Last Event may be the current user event or a subsequest user event or non actionable event.\n",
      "Sample_Value: 2023-04-03 14:55:13.907000>\n",
      "Schema definitions: <Event:A specific occurrence of an action performed by the user at a given time.\n",
      "Activity:Describes a distinct user event, includes both process and non-process events. Only process events can be named. eg. of process events - enter first name, enter address details etc.\n",
      "Abstraction Hierarchy:An application-specific organization of activities logically grouped together into a hierarchical tree structure.\n",
      "Activity Abstraction: Also referred to as task. It is a node in the Abstraction Hierarchy Tree, a chosen logical group of activities within the Abstraction Hierarchy.\n",
      "Activity Instance:A participant-specific occurance of consecutive sequence of events at a given time belonging to the same activity abstraction.\n",
      "Non actionable event:Keyboard or mouse events which does not create user events that can be named are called non-actionable event. eg. alphabets, numbers, mouse scroll, mouse click etc. Also referred as non-process event.\n",
      "Case Identifier:A case will have one more case identifiers which are configured by process owner. Each case identifier has a name and a corresponding value that uniquely identifies the case.\n",
      "Participant: A human that actively engages with the system by performing actions or contributing to the event's occurrence. They can also be referred to as a resource and user. \n",
      "Virtual Assistant: A bot sitting on the user's system that captures the events that observes the participant actions and captures them.\n",
      "\n",
      ">\n",
      "Data table: <../askskan/data/original/sample_data.csv>\n",
      "\n",
      "\n",
      "If there are clarification points and no Spark SQL query, ask the clarification question in this format:\n",
      "    Doubt: <clarification point>\n",
      "\n",
      "After all the clarification points are answered by the human and there's a Spark SQL query generated, use this format:\n",
      "    Output: The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"Query\": {\"title\": \"Query\", \"description\": \"The generated Spark SQL query\", \"type\": \"string\"}, \"Code\": {\"title\": \"Code\", \"description\": \"Python code generated to run the Spark SQL query\", \"type\": \"string\"}, \"Skan_Bot\": {\"title\": \"Skan Bot\", \"description\": \"The final answer printed by the python code in a friendly tone. Place the answer between ##\", \"type\": \"string\"}}, \"required\": [\"Query\", \"Code\", \"Skan_Bot\"]}\n",
      "```\n",
      "\n",
      "Current conversation:\n",
      "Human: Which is the most used application?\n",
      "AI: Doubt: Can you please clarify what you mean by \"most used application\"? Are you asking for the application with the highest active time or the application that was switched to the most frequently?\n",
      "Human: Appliction with highest active time.\n",
      "AI: Doubt: Are you asking for the application with the highest active time in general or for a specific time period?\n",
      "Human: In general\n",
      "AI: Output: The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "```\n",
      "{\n",
      "  \"Query\": \"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\",\n",
      "  \"Code\": \"import pandas as pd\\nfrom pyspark.sql import SparkSession\\n\\nspark = SparkSession.builder.getOrCreate()\\ndata = pd.read_csv('../askskan/data/original/sample_data.csv')\\ndata_table = spark.createDataFrame(data)\\ndata_table.createOrReplaceTempView('data_table')\\n\\nquery = \\\"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\\\"\\nresult = spark.sql(query)\\nresult.show()\\n\",\n",
      "  \"Skan_Bot\": \"The application with the highest active time is <application name> with an active time of <highest active time>.\"\n",
      "}\n",
      "```\n",
      "Human: Which is the most active time window during the day?\n",
      "AI: Doubt: Can you please clarify what you mean by \"active time window\"? Are you asking for the time period during the day with the highest active time?\n",
      "Human: Yes\n",
      "AI: Yes, I am asking for the time period during the day with the highest active time.\n",
      "Human: Yes, the time period during the day with the highest active time.\n",
      "AI: Doubt: Can you please specify the format in which you want the time period during the day to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in between 2 different times. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output format to be in between 2 different times. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in another format. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?   \n",
      "Human: <In what format would you like the time period during the day with the highest active time to be displayed?>\n",
      "Skan Bot: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n"
     ]
    }
   ],
   "source": [
    "result = qa({\"question\": \"I want the output to be in another format.\"})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "Human: Which is the most used application?\n",
      "AI: Doubt: Can you please clarify what you mean by \"most used application\"? Are you asking for the application with the highest active time or the application that was switched to the most frequently?\n",
      "Human: Appliction with highest active time.\n",
      "AI: Doubt: Are you asking for the application with the highest active time in general or for a specific time period?\n",
      "Human: In general\n",
      "AI: Output: The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "```\n",
      "{\n",
      "  \"Query\": \"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\",\n",
      "  \"Code\": \"import pandas as pd\\nfrom pyspark.sql import SparkSession\\n\\nspark = SparkSession.builder.getOrCreate()\\ndata = pd.read_csv('../askskan/data/original/sample_data.csv')\\ndata_table = spark.createDataFrame(data)\\ndata_table.createOrReplaceTempView('data_table')\\n\\nquery = \\\"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\\\"\\nresult = spark.sql(query)\\nresult.show()\\n\",\n",
      "  \"Skan_Bot\": \"The application with the highest active time is <application name> with an active time of <highest active time>.\"\n",
      "}\n",
      "```\n",
      "Human: Which is the most active time window during the day?\n",
      "AI: Doubt: Can you please clarify what you mean by \"active time window\"? Are you asking for the time period during the day with the highest active time?\n",
      "Human: Yes\n",
      "AI: Yes, I am asking for the time period during the day with the highest active time.\n",
      "Human: Yes, the time period during the day with the highest active time.\n",
      "AI: Doubt: Can you please specify the format in which you want the time period during the day to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in between 2 different times. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output format to be in between 2 different times. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in another format. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in another format.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Follow Up Input: I want the output to be in hours and minutes (HH:MM).\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n",
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a super smart code generator.\n",
      "Perform the following actions:\n",
      "\n",
      "1. Understand the input by human, schema and schema definitions each delimited by <>\n",
      "2. Based on the question, the schema and the schema definitions, first seek to clarify any ambiguities.\n",
      "    - Assume human does not know anything about the schema, so never ask details about the schema or schema         definitions or technical details in the clarification points.\n",
      "    - First try to find the answer to the ambiguities from the schema and schema definitions yourself.\n",
      "    - If the ambiguities still persist then: \n",
      "        1. Without mentioning about the schema or schema definitions only think of a list of super short bullets of             areas that need clarification in simple terms. \n",
      "        2. Then pick one clarification point, and wait for an answer from the human before moving to the next point.\n",
      "3. Never ask the human to explain the schema or schema definitions.\n",
      "4. Only human can answer these clarification points.\n",
      "5. Do not assume any schema attributes unless stated explicitly.\n",
      "6. Only after all the clarification points are answered:\n",
      "    - Use only columns in schema to generate a final Spark SQL query answering the original question. Refer to the         schema definitions only for more clarity about terms in the schema.\n",
      "    - Output computationally most efficient Spark SQL query.\n",
      "7. You can ask maximum 7 clarification points.\n",
      "9. I have a pandas csv table in data table delimited by <> that contains the data to be queried.\n",
      "    - Output a valid python code to execute the generated Spark SQL query on the data file ending with a print statement         showing the final answer in a friendly tone.\n",
      "10. Omit any explanations.\n",
      "\n",
      "Schema: <Name: active_time\n",
      "Description: Active time measures the duration when a user is actively interacting with the computer by using the keyboard or mouse between two actions on the computer. It is calculated by dividing the time between the previous and current actions into one-minute intervals. If there is any keyboard or mouse activity within these intervals, that time is counted as part of the active time.\n",
      "Sample_Value: 0\n",
      "\n",
      "Name: processing_time\n",
      "Description: Total idle time below thresh hold-window + active_time (default thresh hold is 3 minutes). Processing time can also be referred to as active_time_in_view (This is different from active_time).\n",
      "Sample_Value: 0.72\n",
      "\n",
      "Name: idle_time\n",
      "Description: Total Time where there is no keyboard or mouse activity within a minute window\n",
      "Sample_Value: 1838.22\n",
      "\n",
      "Name: activity_instance_original_end_time\n",
      "Description: End time is the time of the last event in the Activity Instance. The event can be a user event or non actionable event. Last Event may be the current user event or a subsequest user event or non actionable event.\n",
      "Sample_Value: 2023-04-03 14:55:13.907000>\n",
      "Schema definitions: <Event:A specific occurrence of an action performed by the user at a given time.\n",
      "Activity:Describes a distinct user event, includes both process and non-process events. Only process events can be named. eg. of process events - enter first name, enter address details etc.\n",
      "Abstraction Hierarchy:An application-specific organization of activities logically grouped together into a hierarchical tree structure.\n",
      "Activity Abstraction: Also referred to as task. It is a node in the Abstraction Hierarchy Tree, a chosen logical group of activities within the Abstraction Hierarchy.\n",
      "Activity Instance:A participant-specific occurance of consecutive sequence of events at a given time belonging to the same activity abstraction.\n",
      "Non actionable event:Keyboard or mouse events which does not create user events that can be named are called non-actionable event. eg. alphabets, numbers, mouse scroll, mouse click etc. Also referred as non-process event.\n",
      "Case Identifier:A case will have one more case identifiers which are configured by process owner. Each case identifier has a name and a corresponding value that uniquely identifies the case.\n",
      "Participant: A human that actively engages with the system by performing actions or contributing to the event's occurrence. They can also be referred to as a resource and user. \n",
      "Virtual Assistant: A bot sitting on the user's system that captures the events that observes the participant actions and captures them.\n",
      "\n",
      ">\n",
      "Data table: <../askskan/data/original/sample_data.csv>\n",
      "\n",
      "\n",
      "If there are clarification points and no Spark SQL query, ask the clarification question in this format:\n",
      "    Doubt: <clarification point>\n",
      "\n",
      "After all the clarification points are answered by the human and there's a Spark SQL query generated, use this format:\n",
      "    Output: The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"Query\": {\"title\": \"Query\", \"description\": \"The generated Spark SQL query\", \"type\": \"string\"}, \"Code\": {\"title\": \"Code\", \"description\": \"Python code generated to run the Spark SQL query\", \"type\": \"string\"}, \"Skan_Bot\": {\"title\": \"Skan Bot\", \"description\": \"The final answer printed by the python code in a friendly tone. Place the answer between ##\", \"type\": \"string\"}}, \"required\": [\"Query\", \"Code\", \"Skan_Bot\"]}\n",
      "```\n",
      "\n",
      "Current conversation:\n",
      "Human: Which is the most used application?\n",
      "AI: Doubt: Can you please clarify what you mean by \"most used application\"? Are you asking for the application with the highest active time or the application that was switched to the most frequently?\n",
      "Human: Appliction with highest active time.\n",
      "AI: Doubt: Are you asking for the application with the highest active time in general or for a specific time period?\n",
      "Human: In general\n",
      "AI: Output: The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "```\n",
      "{\n",
      "  \"Query\": \"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\",\n",
      "  \"Code\": \"import pandas as pd\\nfrom pyspark.sql import SparkSession\\n\\nspark = SparkSession.builder.getOrCreate()\\ndata = pd.read_csv('../askskan/data/original/sample_data.csv')\\ndata_table = spark.createDataFrame(data)\\ndata_table.createOrReplaceTempView('data_table')\\n\\nquery = \\\"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\\\"\\nresult = spark.sql(query)\\nresult.show()\\n\",\n",
      "  \"Skan_Bot\": \"The application with the highest active time is <application name> with an active time of <highest active time>.\"\n",
      "}\n",
      "```\n",
      "Human: Which is the most active time window during the day?\n",
      "AI: Doubt: Can you please clarify what you mean by \"active time window\"? Are you asking for the time period during the day with the highest active time?\n",
      "Human: Yes\n",
      "AI: Yes, I am asking for the time period during the day with the highest active time.\n",
      "Human: Yes, the time period during the day with the highest active time.\n",
      "AI: Doubt: Can you please specify the format in which you want the time period during the day to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in between 2 different times. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output format to be in between 2 different times. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in another format. For example, between 9am and 5pm.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n",
      "Human: I want the output to be in another format.\n",
      "AI: Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?   \n",
      "Human: <In what format would you like the time period during the day with the highest active time to be displayed?>\n",
      "Skan Bot: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Doubt: In what format would you like the time period during the day with the highest active time to be displayed? For example, do you want it in hours and minutes (HH:MM) or in another format?\n"
     ]
    }
   ],
   "source": [
    "result = qa({\"question\": \"I want the output to be in hours and minutes (HH:MM).\"})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "spark = SparkSession.builder.getOrCreate()\n",
      "data = pd.read_csv('../askskan/data/original/sample_data.csv')\n",
      "data_table = spark.createDataFrame(data)\n",
      "data_table.createOrReplaceTempView('data_table')\n",
      "\n",
      "query = \"SELECT application, MAX(active_time) AS highest_active_time FROM data_table GROUP BY application ORDER BY highest_active_time DESC LIMIT 1\"\n",
      "result = spark.sql(query)\n",
      "result.show()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code = parser.parse(result['answer']).Code\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = qa({\"question\": \"processing_time field represents the time taken by the total time taken by a participant for all their tasks\"})\n",
    "# print(result['answer'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the output string to look like JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = result['answer']\n",
    "\n",
    "# Find and remove the first occurrence of \"Query\" in the string\n",
    "stripped_string = string.replace('Output:', '', 1).strip()\n",
    "\n",
    "print(stripped_string)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT SUM(processing_time) FROM data_table WHERE agent_type = 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_string = result['answer'] # stripped_string\n",
    "\n",
    "# Remove invalid escape sequences from the JSON string\n",
    "#json_string = json_string.encode('utf-8').decode('unicode_escape')\n",
    "\n",
    "# Convert the modified JSON string to a JSON object\n",
    "json_object = json.loads(json_string, strict=False)\n",
    "\n",
    "# Extract the \"Query\" field\n",
    "query = json_object['Query']\n",
    "\n",
    "# Print the extracted query\n",
    "print(query)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the Python code and answer string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"\n",
    "{\"Query\": [\"SELECT SUM(processing_time) AS total_time FROM data_table WHERE agent_type != 0\"],\n",
    "\"Code\": [\"import pandas as pd\\nfrom pyspark.sql import SparkSession\\n\\nspark = \\\n",
    "    SparkSession.builder.appName('schema').getOrCreate()\\ndata = spark.read.csv('data/sample/data0.csv', \\\n",
    "        header=True, inferSchema=True)\\ndata.createOrReplaceTempView('data_table')\\n\\nquery = \\\n",
    "            \"SELECT SUM(processing_time) AS total_time FROM data_table WHERE agent_type != 0\"\\nresult = \\\n",
    "                spark.sql(query)\\nresult.show()\\nprint('The total time spent on process applications is:', \\\n",
    "                    result.collect()[0][0])\"],\n",
    "\"Skan Bot\": [\"The total time spent on process applications is: #total_time#.\"],\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = \"\"\"\n",
    "{\"Code\": \"import pandas as pd\\nfrom pyspark.sql import SparkSession\\n\\nspark = \\\n",
    "SparkSession.builder.appName('schema').getOrCreate()\\ndata = spark.read.csv('data/sample/data0.csv', \\\n",
    "header=True, inferSchema=True)\\ndata.createOrReplaceTempView('data_table')\\n\\nquery = \\\n",
    "'SELECT SUM(processing_time) AS total_time FROM data_table WHERE agent_type != 0'\\nresult = \\\n",
    "spark.sql(query)\\nresult.show()\\nprint('The total time spent on process applications is:', \\\n",
    "result.collect()[0][0])\"\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "spark = SparkSession.builder.appName('schema').getOrCreate()\n",
      "data = spark.read.csv('data/sample/data0.csv', header=True, inferSchema=True)\n",
      "data.createOrReplaceTempView('data_table')\n",
      "\n",
      "query = 'SELECT SUM(processing_time) AS total_time FROM data_table WHERE agent_type != 0'\n",
      "result = spark.sql(query)\n",
      "result.show()\n",
      "print('The total time spent on process applications is:', result.collect()[0][0])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_string = string1 # result['answer']\n",
    "\n",
    "# Remove invalid escape sequences from the JSON string\n",
    "json_string = json_string.encode('utf-8').decode('unicode_escape')\n",
    "\n",
    "# Find and remove the first occurrence of \"Query\" in the JSON string\n",
    "#json_string = json_string.replace('Query', '', 1).strip()\n",
    "\n",
    "# Convert the modified JSON string to a JSON object\n",
    "json_object = json.loads(json_string, strict=False)\n",
    "\n",
    "# Extract the \"Query\" field\n",
    "code = json_object['Code']\n",
    "#answer = json_object['Skan Bot']\n",
    "\n",
    "# Print the extracted query\n",
    "print(code)\n",
    "#print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    exec(code)\n",
    "except Exception as e:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import sum\n",
    "\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Load the 'data' table into a DataFrame\n",
    "    data = spark.table('data')\n",
    "\n",
    "    # Apply the filter condition\n",
    "    filtered_data = data.filter(data.agent_type != 0)\n",
    "\n",
    "    # Calculate the sum of processing_time\n",
    "    result = filtered_data.select(sum('processing_time').alias('total_time'))\n",
    "\n",
    "    # Show the result\n",
    "    result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "#spark = SparkSession.builder.appName('Query').getOrCreate()\n",
    "\n",
    "# conf = SparkConf().setAppName(\"YourAppName\").setMaster(\"local\").set(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "# spark = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.appName(\"YourAppName\").master(\"local\").config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()\n",
    "\n",
    "data = spark.read.format('csv').option('header', 'true').load('../data/sample/data0.csv')\n",
    "data.createOrReplaceTempView('data0')\n",
    "\n",
    "query = 'SELECT AVG(processing_time) AS avg_processing_time FROM data'\n",
    "avg_processing_time = spark.sql(query).collect()[0]['avg_processing_time']\n",
    "\n",
    "print('The longest average processing time overall is: {} seconds.'.format(avg_processing_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Query').getOrCreate()\n",
    "data = spark.read.format('csv').option('header', 'true').load('../data/sample/data0.csv')\n",
    "data.createOrReplaceTempView('data0')\n",
    "\n",
    "query = 'SELECT participant_id, AVG(processing_time) AS avg_processing_time \\\n",
    "FROM data0 GROUP BY participant_id \\\n",
    "ORDER BY avg_processing_time \\\n",
    "DESC LIMIT 1'\n",
    "\n",
    "result_df = spark.sql(query).toPandas()\n",
    "print('The participant with the longest average processing time is:', result_df['participant_id'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ServiceRequestError",
     "evalue": "<urllib3.connection.HTTPSConnection object at 0x140512820>: Failed to resolve 'model-training' ([Errno 8] nodename nor servname provided, or not known)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServiceRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 31\u001b[0m\n\u001b[1;32m     21\u001b[0m blob_client \u001b[39m=\u001b[39m BlobClient(\n\u001b[1;32m     22\u001b[0m     account_url\u001b[39m=\u001b[39maccount_url, \n\u001b[1;32m     23\u001b[0m     container_name\u001b[39m=\u001b[39mcontainer_name, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     max_chunk_get_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m\u001b[39m*\u001b[39m\u001b[39m1024\u001b[39m\u001b[39m*\u001b[39m\u001b[39m4\u001b[39m \u001b[39m# 4 MiB\u001b[39;00m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../askskan/data/original/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mevents.csv\u001b[39m\u001b[39m'\u001b[39m), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m sample_blob:\n\u001b[0;32m---> 31\u001b[0m     download_stream \u001b[39m=\u001b[39m blob_client\u001b[39m.\u001b[39;49mdownload_blob(max_concurrency\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     32\u001b[0m     sample_blob\u001b[39m.\u001b[39mwrite(download_stream\u001b[39m.\u001b[39mreadall())\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/tracing/decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     78\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/storage/blob/_blob_client.py:914\u001b[0m, in \u001b[0;36mBlobClient.download_blob\u001b[0;34m(self, offset, length, encoding, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Downloads a blob to the StorageStreamDownloader. The readall() method must\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[39mbe used to read all the content or readinto() must be used to download the blob into\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[39ma stream. Using chunks() returns an iterator which allows the user to iterate over the content in chunks.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[39m        :caption: Download a blob.\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    909\u001b[0m options \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_blob_options(\n\u001b[1;32m    910\u001b[0m     offset\u001b[39m=\u001b[39moffset,\n\u001b[1;32m    911\u001b[0m     length\u001b[39m=\u001b[39mlength,\n\u001b[1;32m    912\u001b[0m     encoding\u001b[39m=\u001b[39mencoding,\n\u001b[1;32m    913\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 914\u001b[0m \u001b[39mreturn\u001b[39;00m StorageStreamDownloader(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/storage/blob/_download.py:366\u001b[0m, in \u001b[0;36mStorageStreamDownloader.__init__\u001b[0;34m(self, clients, config, start_range, end_range, validate_content, encryption_options, max_concurrency, name, container, encoding, download_cls, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m     initial_request_end \u001b[39m=\u001b[39m initial_request_start \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_first_get_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    358\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_range, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_offset \u001b[39m=\u001b[39m process_range_and_offset(\n\u001b[1;32m    359\u001b[0m     initial_request_start,\n\u001b[1;32m    360\u001b[0m     initial_request_end,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encryption_data\n\u001b[1;32m    364\u001b[0m )\n\u001b[0;32m--> 366\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initial_request()\n\u001b[1;32m    367\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproperties \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response\u001b[39m.\u001b[39mproperties\n\u001b[1;32m    368\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproperties\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/storage/blob/_download.py:418\u001b[0m, in \u001b[0;36mStorageStreamDownloader._initial_request\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39mwhile\u001b[39;00m retry_active:\n\u001b[1;32m    417\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 418\u001b[0m         location_mode, response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_clients\u001b[39m.\u001b[39;49mblob\u001b[39m.\u001b[39;49mdownload(\n\u001b[1;32m    419\u001b[0m             \u001b[39mrange\u001b[39;49m\u001b[39m=\u001b[39;49mrange_header,\n\u001b[1;32m    420\u001b[0m             range_get_content_md5\u001b[39m=\u001b[39;49mrange_validation,\n\u001b[1;32m    421\u001b[0m             validate_content\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_content,\n\u001b[1;32m    422\u001b[0m             data_stream_total\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    423\u001b[0m             download_stream_current\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m    424\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request_options\n\u001b[1;32m    425\u001b[0m         )\n\u001b[1;32m    427\u001b[0m         \u001b[39m# Check the location we read from to ensure we use the same one\u001b[39;00m\n\u001b[1;32m    428\u001b[0m         \u001b[39m# for subsequent requests.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_location_mode \u001b[39m=\u001b[39m location_mode\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/tracing/decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     78\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/storage/blob/_generated/operations/_blob_operations.py:1592\u001b[0m, in \u001b[0;36mBlobOperations.download\u001b[0;34m(self, snapshot, version_id, timeout, range, range_get_content_md5, range_get_content_crc64, request_id_parameter, lease_access_conditions, cpk_info, modified_access_conditions, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m request \u001b[39m=\u001b[39m _convert_request(request)\n\u001b[1;32m   1590\u001b[0m request\u001b[39m.\u001b[39murl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mformat_url(request\u001b[39m.\u001b[39murl)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m pipeline_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49m_pipeline\u001b[39m.\u001b[39;49mrun(  \u001b[39m# type: ignore # pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m   1593\u001b[0m     request, stream\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m   1594\u001b[0m )\n\u001b[1;32m   1596\u001b[0m response \u001b[39m=\u001b[39m pipeline_response\u001b[39m.\u001b[39mhttp_response\n\u001b[1;32m   1598\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m200\u001b[39m, \u001b[39m206\u001b[39m]:\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:202\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m pipeline_request: PipelineRequest[HTTPRequestType] \u001b[39m=\u001b[39m PipelineRequest(request, context)\n\u001b[1;32m    201\u001b[0m first_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl_policies[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl_policies \u001b[39melse\u001b[39;00m _TransportRunner(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transport)\n\u001b[0;32m--> 202\u001b[0m \u001b[39mreturn\u001b[39;00m first_node\u001b[39m.\u001b[39;49msend(pipeline_request)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "    \u001b[0;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 70 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/policies/_redirect.py:156\u001b[0m, in \u001b[0;36mRedirectPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    154\u001b[0m redirect_settings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfigure_redirects(request\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39moptions)\n\u001b[1;32m    155\u001b[0m \u001b[39mwhile\u001b[39;00m retryable:\n\u001b[0;32m--> 156\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m    157\u001b[0m     redirect_location \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_redirect_location(response)\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m redirect_location \u001b[39mand\u001b[39;00m redirect_settings[\u001b[39m\"\u001b[39m\u001b[39mallow\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/storage/blob/_shared/policies.py:546\u001b[0m, in \u001b[0;36mStorageRetryPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msleep(retry_settings, request\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39mtransport)\n\u001b[1;32m    545\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m    547\u001b[0m \u001b[39mif\u001b[39;00m retry_settings[\u001b[39m'\u001b[39m\u001b[39mhistory\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m    548\u001b[0m     response\u001b[39m.\u001b[39mcontext[\u001b[39m'\u001b[39m\u001b[39mhistory\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m retry_settings[\u001b[39m'\u001b[39m\u001b[39mhistory\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/storage/blob/_shared/policies.py:520\u001b[0m, in \u001b[0;36mStorageRetryPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[39mwhile\u001b[39;00m retries_remaining:\n\u001b[1;32m    519\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m    521\u001b[0m         \u001b[39mif\u001b[39;00m is_retry(response, retry_settings[\u001b[39m'\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m    522\u001b[0m             retries_remaining \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mincrement(\n\u001b[1;32m    523\u001b[0m                 retry_settings,\n\u001b[1;32m    524\u001b[0m                 request\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mhttp_request,\n\u001b[1;32m    525\u001b[0m                 response\u001b[39m=\u001b[39mresponse\u001b[39m.\u001b[39mhttp_response)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "    \u001b[0;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 70 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/storage/blob/_shared/policies.py:313\u001b[0m, in \u001b[0;36mStorageResponseHook.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    308\u001b[0m     upload_stream_current \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mupload_stream_current\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    310\u001b[0m response_callback \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mresponse_callback\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m    311\u001b[0m     request\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mraw_response_hook\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_callback)\n\u001b[0;32m--> 313\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m    315\u001b[0m will_retry \u001b[39m=\u001b[39m is_retry(response, request\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m    316\u001b[0m \u001b[39m# Auth error could come from Bearer challenge, in which case this request will be made again\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/_base.py:101\u001b[0m, in \u001b[0;36m_TransportRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend\u001b[39m(\u001b[39mself\u001b[39m, request: PipelineRequest[HTTPRequestType]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PipelineResponse[HTTPRequestType, HTTPResponseType]:\n\u001b[1;32m     92\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"HTTP transport send method.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \n\u001b[1;32m     94\u001b[0m \u001b[39m    :param request: The PipelineRequest object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m    :rtype: ~azure.core.pipeline.PipelineResponse\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[39mreturn\u001b[39;00m PipelineResponse(\n\u001b[1;32m    100\u001b[0m         request\u001b[39m.\u001b[39mhttp_request,\n\u001b[0;32m--> 101\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sender\u001b[39m.\u001b[39;49msend(request\u001b[39m.\u001b[39;49mhttp_request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49moptions),\n\u001b[1;32m    102\u001b[0m         context\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mcontext,\n\u001b[1;32m    103\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/azure/core/pipeline/transport/_requests_basic.py:364\u001b[0m, in \u001b[0;36mRequestsTransport.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m     error \u001b[39m=\u001b[39m ServiceRequestError(err, error\u001b[39m=\u001b[39merr)\n\u001b[1;32m    363\u001b[0m \u001b[39mif\u001b[39;00m error:\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mraise\u001b[39;00m error\n\u001b[1;32m    365\u001b[0m \u001b[39mif\u001b[39;00m _is_rest(request):\n\u001b[1;32m    366\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mazure\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrest\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_requests_basic\u001b[39;00m \u001b[39mimport\u001b[39;00m RestRequestsTransportResponse\n",
      "\u001b[0;31mServiceRequestError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x140512820>: Failed to resolve 'model-training' ([Errno 8] nodename nor servname provided, or not known)"
     ]
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential \n",
    "from azure.storage.blob import BlobClient\n",
    "\n",
    "# Storage Account name\n",
    "account_name = os.environ['DATALAKE_ACCOUNT_NAME']\n",
    "# Storage Account key\n",
    "account_key = os.environ['DATALAKE_ACCOUNT_KEY']\n",
    "\n",
    "# Name of the container where the blob is stored\n",
    "container_name = \"model-training\"\n",
    "\n",
    "# Name of the blob you want to fetch\n",
    "blob_name = \"DataShare/data1/2023-03-01-to-2023-03-31/events/events.csv\"\n",
    "\n",
    "file_path = \"../data/original/\"\n",
    "file_name = \"events.csv\"\n",
    "\n",
    "\n",
    "account_url = \"https://{account_key}.blob.core.windows.net\"\n",
    "# Create a BlobClient object with data transfer options for download\n",
    "blob_client = BlobClient(\n",
    "    account_url=account_url, \n",
    "    container_name=container_name, \n",
    "    blob_name=blob_name,\n",
    "    credential=account_key, #DefaultAzureCredential(),\n",
    "    max_single_get_size=1024*1024*32, # 32 MiB\n",
    "    max_chunk_get_size=1024*1024*4 # 4 MiB\n",
    ")\n",
    "\n",
    "with open(file=os.path.join(r'../askskan/data/original/', 'events.csv'), mode=\"wb\") as sample_blob:\n",
    "    download_stream = blob_client.download_blob(max_concurrency=2)\n",
    "    sample_blob.write(download_stream.readall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
