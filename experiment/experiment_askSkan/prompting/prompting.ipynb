{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring what kind of prompts would affect the kind of output the ChatGPT API would give out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saishradhamohanty/Desktop/Repo/Prototypes/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix this\n",
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key  =  os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message[\"content\"]\n",
    "    token_dict = {\n",
    "        'prompt_tokens':response['usage']['prompt_tokens'],\n",
    "        'completion_tokens':response['usage']['completion_tokens'],\n",
    "        'total_tokens':response['usage']['total_tokens'],\n",
    "    }\n",
    "    return content, token_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Principle 1 for prompting:\n",
    "1. Use delimiters to specify the part of the prompt that requires action.\n",
    "2. Ask the model to give the out in a specific structured format.\n",
    "3. Ask the model to check for specific conditions in the input text in prompt and avoid any assumptions.\n",
    "4. Perform few-shot prompting by providing the model with example conversation. -> *How to do this in case of code generation?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Context,Question ,Answer,Theme\n",
    "The dataset contains information about tasks completed by different participants in the claim process. This includes details like processing time, applications used, and number of keystrokes.\n",
    "Who is the most productive participant based on the number of tasks completed?\n",
    "The participant with the most tasks completed is John.\n",
    "Productivity\n",
    "\n",
    "The processing time refers to the time spent by a participant on a particular task. This could potentially be an indicator of the participant's efficiency.\n",
    "Who has the shortest average processing time?\n",
    "The participant with the shortest average processing time is Mary.\n",
    "Productivity\n",
    "\n",
    "The dataset includes the number of keystrokes made by the participants during each task.\n",
    "Who uses the least number of keystrokes on average?\n",
    "The participant with the least average number of keystrokes is Steve.\n",
    "Productivity\n",
    "\n",
    "The 'applications' field lists the applications used by a participant for a task. The use of multiple applications may indicate multitasking ability or complexity of the task.\n",
    "Which participant uses the most applications on average?\n",
    "John uses the most applications on average.\n",
    "Productivity\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. case_id : this is a unique text identifier which represents a unique process execution. For example, a claim number or a policy number or a customer number. \n",
      "2. case_status: this text identifier represents the status of case during the row observation. For example case_status may be open, in_progress, on_hold, closed, updated e.t.c. This field may not be available in some cases where it is not indicated on an agent/user's screen during observation. \n",
      "3. case_type: this text identifier represents the type of case being handled during the row observation. For instance, case_type may be billing, renewal, cancellation, quick_claims e.t.c depending on the case categories in the organization. This field may not be available in some cases where it is not indicated on an agent/user's screen during observation. \n",
      "4. task_name: this text field represents a business friendly name given to the action being performed by an agent/user/participant such that the context of this action is easily understood. For example, the task_name \"update customer\" could comprise several individual actions like \"navigating to home menu\", \"double clicking on email app icon\", \"composing email\", \"attaching document from other application\" and \"sending email to customer\". \n",
      "5. participant_id: this text field contains the name of a participant (or agent or user) who is performing the task for the row in question. For example, a participant_id may be julian, meryl, goran, leah e.t.c \n",
      "6. process_name: this text field represents the process under which the task is performed. For example, the task_name  \"update_customer\" could be under the process_name \"complaints_handling\" \n",
      "7. process_variant: this text field represents a functional variation under the process in question. For example, the process_name \"complaints_handling\" might be performed largely in 8 different ways. In this example, we would assign the process_variant for the concerned row a value of 1 to 8 based on its related process_variant. This field is usually not available and this is something I would like for the LLM to become very adept at estimating based on the other fields provided. \n",
      "8. applications_used: this is a text array, containing a string of applications that were used during the task in chronological order. For example, the participant may have opened outlook, then used notepad and then back to outlook. In this case, applications used would be : [outlook, notepad, outlook] \n",
      "9. processing_time: this is the time in seconds where a participant is actively working on a task. It can also be looked upon as active time. \n",
      "10. wait_time: this is the time in seconds where a participant is not activelu working on a task. It starts to count from when no action has been taken in the last three minutes. \n",
      "11. turnaround_time: this time in seconds is the processing_time plus the wait_time. \n",
      "12. start_time: this is the time in UTC format where a participant first actively begins a task. The start_time plus the turnaround_time should lead you to the time the task ended. \n",
      "13. no_of_keystokes: this is the count of the number of times a participant/agent/user peforms a keystroke on their computer. The idea is for the LLM to use this to identify automation opportunities. \n",
      "14. no_of_shortcut_keys: this is the count of the number of times a participant/agent/user peforms a shortcut on their computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the text file\n",
    "file_path = '../data/sample/schema0.txt'\n",
    "\n",
    "# Read the contents of the text file\n",
    "with open(file_path, 'r') as file:\n",
    "    file_contents = file.read()\n",
    "\n",
    "# Print the contents of the text file\n",
    "print(file_contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = f\"\"\"Who has the longest average processing time?\"\"\"\n",
    "question_2 = f\"\"\"Who has the shortest average processing time?\"\"\"\n",
    "question_3 = f\"\"\"Who is the most productive participant based on the number of tasks completed?\"\"\"\n",
    "question_4 = f\"\"\"Who uses the least number of keystrokes on average?\"\"\"\n",
    "question_5 = f\"\"\"Name of the participant that uses the most number of long cut keys on average?\"\"\" # An invalid question, to check hallucination\n",
    "schema = file_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for prompt_1: 2.034454107284546\n",
      "\n",
      "Completion for prompt 1:\n",
      "SQL query: <SELECT participant_id, AVG(turnaround_time) AS avg_processing_time FROM table_name GROUP BY participant_id ORDER BY avg_processing_time DESC LIMIT 1;>\n",
      "\n",
      "Token dict for prompt 1:\n",
      "{'prompt_tokens': 877, 'completion_tokens': 35, 'total_tokens': 912}\n"
     ]
    }
   ],
   "source": [
    "# ask model to generate code in SQL by looking at the schema and the input questions\n",
    "prompt_1 = f\"\"\" Consider yourself to be a SQL code generator.\n",
    "            Your task is to perform the following actions:\n",
    "            1 - Read and understand the following question delimited by <>.\n",
    "            2 - Read and understand the following schema delimited by <>.\n",
    "            3 - Use the schema to generate correct SQL query for the question.\n",
    "            4 - Output correct SQL query.\n",
    "\n",
    "            Use the following format:\n",
    "            Question: <User question for which SQL query has to be generated>\n",
    "            Schema: <Schema of the database table according to which generate the correct SQL query for the given question>\n",
    "            SQL query: <valid SQL code>\n",
    "\n",
    "            Question: <{question_1}>\n",
    "            Schema: <{schema}>\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "response_1, token_dict_1 = get_completion(prompt_1)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken for prompt_1:\", end-start)\n",
    "print(\"\\nCompletion for prompt 1:\")\n",
    "print(response_1)\n",
    "print(\"\\nToken dict for prompt 1:\")\n",
    "print(token_dict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for prompt_2: 2.3985209465026855\n",
      "\n",
      "Completion for prompt 2:\n",
      "SQL query: SELECT participant_id, AVG(turnaround_time) as avg_processing_time FROM table_name GROUP BY participant_id ORDER BY avg_processing_time DESC LIMIT 1\n",
      "\n",
      "Token dict for prompt 2:\n",
      "{'prompt_tokens': 889, 'completion_tokens': 32, 'total_tokens': 921}\n"
     ]
    }
   ],
   "source": [
    "# ask model to generate code in SQL by looking at the schema and the input questions\n",
    "prompt_2 = f\"\"\" Consider yourself to be a SQL code generator.\n",
    "            Your task is to perform the following actions:\n",
    "            1 - Read and understand the following question delimited by <>.\n",
    "            2 - Read and understand the following schema delimited by <>.\n",
    "            3 - Use the schema to generate correct SQL query for the question.\n",
    "            4 - Output correct SQL query.\n",
    "            5 - Omit any explanation in the end.\n",
    "\n",
    "            Use the following format:\n",
    "            Question: <User question for which SQL query has to be generated>\n",
    "            Schema: <Schema of the database table according to which generate the correct SQL query for the given question>\n",
    "            SQL query: <valid SQL code>\n",
    "\n",
    "            Question: <{question_1}>\n",
    "            Schema: <{schema}>\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "response_2, token_dict_2 = get_completion(prompt_2)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken for prompt_2:\", end-start)\n",
    "print(\"\\nCompletion for prompt 2:\")\n",
    "print(response_2)\n",
    "print(\"\\nToken dict for prompt 2:\")\n",
    "print(token_dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for prompt_3: 2.9995551109313965\n",
      "\n",
      "Completion for prompt 3:\n",
      "SQL query: \n",
      "\n",
      "SELECT participant_id, AVG(turnaround_time) as avg_process_time \n",
      "FROM table_name \n",
      "GROUP BY participant_id \n",
      "ORDER BY avg_process_time DESC \n",
      "LIMIT 1;\n",
      "\n",
      "Token dict for prompt 3:\n",
      "{'prompt_tokens': 870, 'completion_tokens': 38, 'total_tokens': 908}\n"
     ]
    }
   ],
   "source": [
    "# ask model to generate code in SQL by looking at the schema and the input questions\n",
    "prompt_3 = f\"\"\" You are a SQL code generator.\n",
    "            Perform the following actions:\n",
    "            1 - Understand the question delimited by <>.\n",
    "            2 - Understand the schema delimited by <>.\n",
    "            3 - Use the schema to generate SQL query for the question.\n",
    "            4 - Output correct SQL query.\n",
    "            5 - Omit any explanation in the end.\n",
    "\n",
    "            Use the following format:\n",
    "            Question: <Question for which SQL query has to be generated>\n",
    "            Schema: <Schema of the database table to be queried for the given question>\n",
    "            SQL query: <valid SQL query>\n",
    "\n",
    "            Question: <{question_1}>\n",
    "            Schema: <{schema}>\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "response_3, token_dict_3 = get_completion(prompt_3)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken for prompt_3:\", end-start)\n",
    "print(\"\\nCompletion for prompt 3:\")\n",
    "print(response_3)\n",
    "print(\"\\nToken dict for prompt 3:\")\n",
    "print(token_dict_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for prompt_4: 2.4593677520751953\n",
      "\n",
      "Completion for prompt 4:\n",
      "SQL query: SELECT participant_id, AVG(turnaround_time) AS avg_processing_time\n",
      "            FROM table_name\n",
      "            GROUP BY participant_id\n",
      "            ORDER BY avg_processing_time DESC\n",
      "            LIMIT 1;\n",
      "\n",
      "Token dict for prompt 4:\n",
      "{'prompt_tokens': 867, 'completion_tokens': 41, 'total_tokens': 908}\n"
     ]
    }
   ],
   "source": [
    "# ask model to generate code in SQL by looking at the schema and the input questions\n",
    "prompt_4 = f\"\"\" You are a SQL code generator.\n",
    "            Perform the following actions:\n",
    "            1 - Understand the question delimited by <>.\n",
    "            2 - Understand the schema delimited by <>.\n",
    "            3 - Use the schema to generate SQL query answering the question.\n",
    "            4 - Output correct SQL query.\n",
    "            5 - Omit any explanations.\n",
    "\n",
    "            Use the following format:\n",
    "            Question: <Question to be answered by the generated SQL query>\n",
    "            Schema: <Schema of the database table to be queried for the given question>\n",
    "            SQL query: <valid SQL query>\n",
    "\n",
    "            Question: <{question_1}>\n",
    "            Schema: <{schema}>\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "response_4, token_dict_4 = get_completion(prompt_4)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken for prompt_4:\", end-start)\n",
    "print(\"\\nCompletion for prompt 4:\")\n",
    "print(response_4)\n",
    "print(\"\\nToken dict for prompt 4:\")\n",
    "print(token_dict_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for prompt_5: 2.0653066635131836\n",
      "\n",
      "Completion for prompt 5:\n",
      "SQL query: \n",
      "\n",
      "SELECT participant_id \n",
      "FROM table_name \n",
      "GROUP BY participant_id \n",
      "ORDER BY AVG(turnaround_time) DESC \n",
      "LIMIT 1\n",
      "\n",
      "Token dict for prompt 5:\n",
      "{'prompt_tokens': 870, 'completion_tokens': 29, 'total_tokens': 899}\n"
     ]
    }
   ],
   "source": [
    "# ask model to generate code in SQL by looking at the schema and the input questions\n",
    "prompt_5 = f\"\"\" You are a SQL code generator.\n",
    "            Perform the following actions:\n",
    "            1 - Understand the question delimited by <>.\n",
    "            2 - Understand the schema delimited by <>.\n",
    "            3 - Use the schema to generate SQL query answering the question.\n",
    "            4 - Output computationally most efficient SQL query.\n",
    "            5 - Omit any explanations.\n",
    "\n",
    "            Use the following format:\n",
    "            Question: <Question to be answered by the generated SQL query>\n",
    "            Schema: <Schema of the database table to be queried for the given question>\n",
    "            SQL query: <valid SQL query>\n",
    "\n",
    "            Question: <{question_1}>\n",
    "            Schema: <{schema}>\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "response_5, token_dict_5 = get_completion(prompt_5)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken for prompt_5:\", end-start)\n",
    "print(\"\\nCompletion for prompt 5:\")\n",
    "print(response_5)\n",
    "print(\"\\nToken dict for prompt 5:\")\n",
    "print(token_dict_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for prompt_6: 2.3339860439300537\n",
      "\n",
      "Completion for prompt 6:\n",
      "SQL query: \n",
      "\n",
      "SELECT participant_id \n",
      "FROM table_name \n",
      "GROUP BY participant_id \n",
      "ORDER BY AVG(turnaround_time) DESC \n",
      "LIMIT 1\n",
      "\n",
      "Token dict for prompt 6:\n",
      "{'prompt_tokens': 891, 'completion_tokens': 29, 'total_tokens': 920}\n"
     ]
    }
   ],
   "source": [
    "# ask model to generate code in SQL by looking at the schema and the input questions\n",
    "# AVOID ASSUMPTIONS IN THE MODEL NOW\n",
    "prompt_6 = f\"\"\" You are a SQL code generator.\n",
    "            Perform the following actions:\n",
    "            1 - Understand the question delimited by <>.\n",
    "            2 - Understand the schema delimited by <>.\n",
    "            3 - Do not assume anything, if there's an ambiguity, ask a followup question.\n",
    "            4 - Use the schema to generate SQL query answering the question.\n",
    "            5 - Output computationally most efficient SQL query.\n",
    "            6 - Omit any explanations.\n",
    "\n",
    "            Use the following format:\n",
    "            Question: <Question to be answered by the generated SQL query>\n",
    "            Schema: <Schema of the database table to be queried for the given question>\n",
    "            SQL query: <valid SQL query>\n",
    "\n",
    "            Question: <{question_1}>\n",
    "            Schema: <{schema}>\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "response_6, token_dict_6 = get_completion(prompt_6)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken for prompt_6:\", end-start)\n",
    "print(\"\\nCompletion for prompt 6:\")\n",
    "print(response_6)\n",
    "print(\"\\nToken dict for prompt 6:\")\n",
    "print(token_dict_6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "1. Question 1 \n",
    "- Prompt 1: is open ended -> output can also have the reasoning followed by the model.\n",
    "- Prompt 2: no reasoning, -> lesser response time by a few msec/sec -> more prompt tokens -> less cost effective as per speedup.\n",
    "- Prompt 3: lesser prompt tokens and response time that prior ones (*if model uses processing time instead of turnaround time*).\n",
    "- Prompt 4: least prompt tokens and response time (*if model uses processing time instead of turnaround time*).\n",
    "\n",
    "In the previous prompts the model **assumes** (*the model sometimes uses processing time and sometimes turnaround time*) -> *need to avoid assumptions*. Assumes that processing time includes the wait time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Principle 2 for prompting:\n",
    "1. Specify the steps to complete a task. -> *It might require more tokens as input, how to deal with that?*\n",
    "2. Instruct the model to work out it's own solution before rushing to a conclusion. -> *How to do this in our use case where time is important?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prompting on the Web UI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge Cases:\n",
    "1. Assumptions by the model\n",
    "    - **Missing variables:** To prevent the model from making up variables on its own, instruct the model to not assume the presence of those variables and ask followup questions in case something is missing.\n",
    "    - **Avoids reading through the schema descriptions carefully:** Sometimes the model assumes a few things based on past data trends without reading properly through the schema descriptions.\n",
    "\n",
    "2. Hallucination\n",
    "    - **Reasoning based on past knowledge and experience**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim:\n",
    "Improve prompts such that the model can ask correct follow-up questions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the following actions using the same schema and different question:\n",
    "- If schema and/or question is missing, ask a followup question asking for schema and/or question in the input.\n",
    "- Understand the question and schema each delimited by <>.\n",
    "- If the any row in the schema or any combination of rows in the schema don't have information for answering the question, ask for more information from the user.\n",
    "- Use the schema to generate SQL query answering the question.\n",
    "- Output computationally most efficient SQL query.\n",
    "- Omit any explanations.\n",
    "\n",
    "            Use the following format:\n",
    "            Question: <Question to be answered by the generated SQL query>\n",
    "            Schema: <Schema of the database table to be queried for the given question>\n",
    "            SQL query: <valid SQL query>\n",
    "\n",
    "Question: Which participant uses the most applications on average?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
